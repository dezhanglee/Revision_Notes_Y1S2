\PassOptionsToPackage{svgnames}{xcolor}
\documentclass[12pt]{article}



\usepackage[margin=1in]{geometry}  
\usepackage{graphicx}             
\usepackage{amsmath}              
\usepackage{amsfonts}              
\usepackage{framed}               
\usepackage{amssymb}
\usepackage{array}
\usepackage{amsthm}
\usepackage[nottoc]{tocbibind}
\usepackage{bm}
\usepackage[object=vectorian]{pgfornament} 
\usepackage{enumitem}

\colorlet{shadecolor}{lightgray!25}
\newcommand{\sectionline}{%
  \noindent
  \begin{center}
  {\color{DarkViolet}
    \resizebox{0.5\linewidth}{1ex}
    {{%
    {\begin{tikzpicture}
    \node  (C) at (0,0) {};
    \node (D) at (9,0) {};
    \path (C) to [ornament=85] (D);
    \end{tikzpicture}}}}}%
    \end{center}
  }
\DeclareMathOperator{\Tr}{Tr}
 \newcommand{\im}{\mathrm{i}}
  \newcommand{\diff}{\mathrm{d}}
  \newcommand{\col}{\mathrm{Col}}
  \newcommand{\row}{\mathrm{R}}
  \newcommand{\kerne}{\mathrm{Ker}}
  \newcommand{\nul}{\mathrm{Null}}
  \newcommand{\nullity}{\mathrm{nullity}}
  \newcommand{\rank}{\mathrm{rank}}
  \newcommand{\Hom}{\mathrm{Hom}}
  \newcommand{\id}{\mathrm{id}}
  \newcommand{\ima}{\mathrm{Im}}
  \newcommand{\lcm}{\mathrm{lcm}}
  \newcommand\norm[1]{\left\lVert#1\right\rVert}
\setlength{\parindent}{0cm}
\setlength{\parskip}{0em}
\newcommand{\Lim}[1]{\raisebox{0.5ex}{\scalebox{0.8}{$\displaystyle \lim_{#1}\;$}}}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{notation}{Notation}[section]
\theoremstyle{definition}
\DeclareMathOperator{\arcsec}{arcsec}
\DeclareMathOperator{\arccot}{arccot}
\DeclareMathOperator{\arccsc}{arccsc}
\DeclareMathOperator{\spn}{Span}
\setcounter{tocdepth}{1}
\begin{document}

\title{Revision notes - MA2101}
\author{Ma Hongqiang}
\maketitle
\tableofcontents

\clearpage
%\twocolumn
\section{Vector Spaces over a Field}
\begin{definition}[Field, Rings, Groups]
\hfill\\
\normalfont Let $\mathbb{F}$ be a set containing at least two elements and equipped with the following two binary operations $+$(the addition, or plus) and $\times$(the multiplication, or times), where $\mathbb{F}\times\mathbb{F}:=\{(x,y)\mid x,y\in\mathbb{F}\}$ is the \textbf{product set} of $\mathbb{F}$ with iteself:
\[
\begin{aligned}
+:&\mathbb{F}\times\mathbb{F}\to\mathbb{F}\\
&(x,y)\mapsto x+y;\\
\times :&\mathbb{F}\times\mathbb{F}\to\mathbb{F}\\
&(x,y)\mapsto x\times y;\\
\end{aligned}
\]
\textbf{Axiom(0)} Addition and multiplication are well defined on $\mathbb{F}$ in the sense that:
\[
\begin{aligned}
&\forall x\in\mathbb{F}, \forall y\in\mathbb{F}\Rightarrow x+y\in\mathbb{F}\\
&\forall x\in\mathbb{F}, \forall y\in\mathbb{F}\Rightarrow xy\in\mathbb{F}
\end{aligned}
\]
Namely, the operation map $+$(resp. $\times$) takes element $(x,y)$ in the domain $\mathbb{F}\times\mathbb{F}$ to some element $x+y$(resp. $xy$) in the codomain $\mathbb{F}$.
The quintuple $(\mathbb{F},+,0;\times,1)$ with two distinguished elements $0$ (the additive identity) and $1$ (the multiplicative identity), is called a \textbf{field} if the following \textbf{Eight Axioms} (and also \textbf{Axiom (0)}) are satisfied.
\begin{enumerate}[label=(\arabic*)]
\item Existence of an \textbf{additive identity} $0_{\mathbb{F}}$ or simply $0$:
\[
x+0=x=0+x,\forall x\in\mathbb{F}
\]
\item (Additive) Associativity:
\[
(x+y)+z=x+(y+z),\forall x,y,z\in\mathbb{F}
\]
\item Additive Inverse\\for every $x\in\mathbb{F}$, there is an \textbf{additive inverse} $-x\in\mathbb{F}$ of $x$ such that 
\[
x+(-x)=0=(-x)+x
\]
\item Existence of a \textbf{multiplicative identity} $1_\mathbb{F}$ or simply $1$:
\[
x1=x=1x,\forall x\in\mathbb{F}
\]
\item (Multiplicative) Associativity:
\[
(xy)z=x(yz),\forall x,y,z\in\mathbb{F}
\]
\item Multiplicative Inverse for nonzero element:\\for every $0\neq x\in \mathbb{F}$, there is a \textbf{multiplicative inverse} $x^{-1}\in\mathbb{F}$ such that 
\[xx^{-1}=1=x^{-1}x.\]
\item Distributive Laww:
\[
\begin{aligned}
(x+y)z&=xz+yz,\forall x,y,z\in\mathbb{F}\\
z(x+y)&=zx+zy,\forall x,y,z\in\mathbb{F}
\end{aligned}
\]
\item Commutativity for addition and multiplication:
\[
\begin{aligned}
x+y&=y+x,&\forall x,y\in\mathbb{F}\\
xy&=yx,&\forall x,y\in\mathbb{F}
\end{aligned}
\]
\end{enumerate}
The triplet $(\mathbb{F},+,\times)$ with only the Axioms (1)---(5) and (7)---(8) satisfied is called a (commutative) ring.\\
The pair $(\mathbb{F},+)$ with only Axioms (1)---(3) satisfied by its binary operation $+$, is called an (additive) group.
\end{definition}
\begin{notation}[about $\mathbb{F}^{\times}$]\normalfont For a field $(\mathbb{F},+,0;\times, 1)$, we use $\mathbb{F}^\times$ to denote the set of nonzero elements in $\mathbb{F}$:
\[
\mathbb{F}^\times :=\mathbb{F}\setminus\{0\}
\]
\end{notation}
\begin{definition}[Polynomial ring]
\hfill\\
\normalfont Let $(\mathbb{F},+,0;\times, 1)$ be a field (or a ring), e.g. $\mathbb{F}=\mathbb{Z},\mathbb{Q},\mathbb{R},\mathbb{C}$.
\[
g(x)=\sum^n_{i=0} a_i x^i = a_n x^n + a_{n-1}x^{n-1}+\cdots+a_1 x+a_0
\]
with the \textbf{leading coefficient} $a_n\neq 0$, is called a \textbf{polynomial of degree} $n\geq 0$, in one variable $x$ and with coefficients $a_i\in\mathbb{F}$. Let
\[
\mathbb{F}[x]:=\{\sum_{j=0}^d b_j x^j\mid d\geq 0, b_j\in\mathbb{F}\}
\]
be the set of all polynomials in one variable $x$ and with coefficients in $\mathbb{F}$.
\end{definition}
\begin{theorem}[Uniqueness of identity and inverse]\normalfont Let $\mathbb{F}$ be a field.
\begin{enumerate}[label=(\arabic*)]
\item $\mathbb{F}$ has only one additive identity $0$.
\item $\mathbb{F}$ has only one multiplicative identity $1$.
\item Every $x\in\mathbb{F}$ has only one additive inverse $-x$.
\item Every $x\in\mathbb{F}^\times$ has only one multiplicative inverse $x^{-1}$.
\end{enumerate}
\end{theorem}
\begin{theorem}[Properties of $\mathbb{F}$]\hfill\\\normalfont
\begin{enumerate}[label=(\arabic*)]
\item (Cancelation Law) Let $b,x,y\in\mathbb{F}$. Then
\[
b+x=b+y\Rightarrow x=y
\]
\item (Killing Power of $0$)
\[
0x=0=x0,\forall x\in\mathbb{F}
\]
\item In $\mathbb{F}$, we have
\[
0_\mathbb{F}\neq 1_\mathbb{F}
\]
\item If $x\in\mathbb{F}^{\times}$, then its multiplicative inverse
\[
x^{-1}\in\mathbb{F}^\times
\]
\item If $x+x^\prime=0$, then $x$ and $x^\prime$ are multiplicative inverse to each other.
\item If $xx^{\prime\prime} = 1$ then $x$ and $x^{\prime\prime}$ are multiplicative inverse to each other.
\end{enumerate}
\end{theorem}
\begin{definition}[Vector Space]
\hfill\\
\normalfont Let $\mathbb{F}$ be a field and $V$ a non-empty set, with a binary vector addition operation
\[
\begin{aligned}
+ : V\times V&\to V\\
(\mathbf{v}_1,\mathbf{v}_2)&\mapsto \mathbf{v}_1+\mathbf{v}_2
\end{aligned}
\]
and scalar multiplication operation
\[
\begin{aligned}
\times : V\times V&\to V\\
(c,\mathbf{v})&\mapsto c\mathbf{v}
\end{aligned}
\]
\textbf{Axiom(0)} These two operations are well defined in the sense that
\[
\begin{aligned}
\forall \mathbf{v}_i\in V&\Rightarrow \mathbf{v}_1+\mathbf{v}_2\in V\\
\forall c\in\mathbb{F}, \forall \mathbf{v}\in V&\Rightarrow c\mathbf{v}\in V
\end{aligned}
\]
$(V,+)$ is a \textbf{vector space over the field} $\mathbb{F}$ if the following \textbf{Seven Axioms} are satisfied.
\begin{enumerate}[label=(\arabic*)]
\item Existence of \textbf{zero vector} $0_V$:
\[
\mathbf{v}+\mathbf{0}=\mathbf{v}=\mathbf{v}+\mathbf{0}, \forall \mathbf{v}\in V
\]
\item (Additive) Associativity:
\[
(\mathbf{u}+\mathbf{v})+\mathbf{w}= \mathbf{u}+(\mathbf{v}+\mathbf{w}), \forall \mathbf{u},\mathbf{v},\mathbf{w}\in V
\]
\item Additive Inverse:\\for every $\mathbf{v}\in V$, there is an additive inverse $-\mathbf{v}$ of $\mathbf{v}$ such that
\[
\mathbf{v}+(-\mathbf{v})=\mathbf{0}=(-\mathbf{v})+\mathbf{v}
\]
\item The effect of $1\in \mathbb{F}$ on $V$:
\[
1\mathbf{v}=\mathbf{v}, \forall \mathbf{v}\in V
\]
\item (Multiplicative) Associativity:
\[
(ab)\mathbf{v}=a(b\mathbf{v}),\forall a,b\in \mathbb{F},\mathbf{v}\in V
\]
\item Distributive Law:
\[
\begin{aligned}
(a+b)\mathbf{v}&=a\mathbf{v}+b\mathbf{v},\forall a,b\in \mathbb{F}, \mathbf{v}\in V\\
a(\mathbf{u}+\mathbf{v})&=a\mathbf{u}+a\mathbf{v}, \forall a\in \mathbb{F}, \mathbf{u},\mathbf{v}\in V
\end{aligned}
\]
\item Commutativity for the vector addition:
\[
\mathbf{u}+\mathbf{v}=\mathbf{v}+\mathbf{u}, \forall \mathbf{u},\mathbf{v}\in V
\]
\end{enumerate}
\end{definition}
\textbf{Remark:} Every vector space $V$ contains the zero vector $\mathbf{0}_V$.
\clearpage
\section{Vector Subspaces}
\begin{definition}[Subspace]
\hfill\\
\normalfont Let $V$ be a vector space over a field $\mathbb{F}$. A non-empty subset $W\subseteq V$ is called a \textbf{vector subspace of} $V$ if the following two conditions are satisfied:
\begin{itemize}
\item[(CA)] (\textbf{C}losed under vector \textbf{A}ddition) 
\[
\forall \mathbf{w}_i\in W\Rightarrow \mathbf{w}_1+\mathbf{w}_2\in W
\]
\item[(CS)] (\textbf{C}losed under \textbf{S}calar Multiplication)
\[
\forall a\in \mathbb{F}, \forall\mathbf{w}\in W\Rightarrow a\mathbf{w}\in W
\] 
\end{itemize}
\end{definition}
Obvious subspace of $V$ are $V$ and $\{\mathbf{0}\}$.\\
\textbf{Remark:} Every vector subspace $W$ of $V$ contains the zero vector $\mathbf{0}_W$.
\begin{definition}[Linear Combination]
\hfill\\\normalfont Let $V$ be a vector space over a field $\mathbb{F}$ and $W\subseteq V$ a non-empty subset. Then the following are equivalent:
\begin{enumerate}[label=(\roman*)]
\item $W$ is a vector subspace of $V$, i.e. $W$ is closed under vector addition and scalar multiplication, in the sense of Definition of Subspace.
\item $W$ is closed under linear combination:
\[
\forall a_o\in\mathbb{F}, \forall\mathbf{w}_i\in W\Rightarrow a_1\mathbf{w}_1+a_2\mathbf{w}_2\in W
\]
\item $W$ together with the vector addition $+$ and the scalar multiplication $\times$, becomes a vector space.
\end{enumerate}
\end{definition}
\begin{theorem}[Intersection of Subspace being a subspace]
\hfill\\\normalfont Let $V$ be a vector space over a field $\mathbb{F}$ and let $W_\alpha\subseteq V(\alpha\in I)$ be vector subspaces of $V$. Then the intersection
\[
\cap_{\alpha\in I}W_\alpha
\]
is again a vector subspace of $V$
\end{theorem}
\textbf{Remark:} Union of subspaces may not be a subspace. However, union of subspaces is closed under scalar multiplication.\\
\clearpage
\section{Linear Spans and Direct Sums of Subspaces}
\begin{definition}[Linear combination and Linear Span]
\hfill\\\normalfont Let $V$ be a vector space over a field $\mathbb{F}$. A vector $\mathbf{v}\in V$ is called a \textbf{linear combination of some vectors} $\mathbf{v}_i\in V(1\leq i\leq s)$ if
\[
\mathbf{v} = a_1\mathbf{v}_1+a_2\mathbf{v}_2+\cdots + a_s\mathbf{v}_s
\]
for some scalars $a_i\in\mathbb{F}$.
Let $S\subseteq V$ be a non-empty subset. The subset $\spn(S):=$
\[
\{\mathbf{v}\in V\mid \mathbf{v}\text{ is a linear combination of some vectors in }S\}
\]
of $V$ is called the \textbf{vector subspace of }$V$\textbf{spanned by the subset }$S$.
\end{definition}
\begin{theorem}[Span being a subspace]
\hfill\normalfont
\begin{enumerate}[label=(\roman*)]
\item The subset $\spn(S)$ of $V$ is indeed a vector subspace of $V$.
\item $\spn(S)$ is the smallest vector subspace of $V$ containing the set $S$:\\
firstly, $\spn(S)$ is a vector subspace of $V$ containing $S$;\\
secondly, if $W$ is another vector subspace of $V$ containing $S$, then $W\supseteq\spn(S)$
\end{enumerate}
\end{theorem}
\begin{definition}[Sum of subspaces]
\hfill\\
\normalfont Let $V$ be a vector space over a field $\mathbb{F}$, and let $U$ and $W$ be vector subspaces of $V$. The subset
\[
U+W={\mathbf{u}+\mathbf{w}\mid \mathbf{u}\in U, \mathbf{w}\in W}
\]
is called the \textbf{sum of the subspaces }$U$ and $W$. 
\end{definition}
\begin{theorem}[Sum being a subspace] 
\hfill\\\normalfont Let $U$ and $W$ be vector subspaces of a vector space $V$ over a field $\mathbb{F}$. For the sum $U+W$, we have:
\begin{enumerate}
\item $U+W=\spn(U\cup W)$
\item $U+W$ is indeed a vector subspace of $V$.
\item $U+W$ is the smallest vector subspace of $V$ containing both $U$ and $W$:\\
first, $U+W$ is a vector subspace of $V$ containing both $U$ and $W$;
secondly, if $T$ is another vector subspace of $V$ containing both $U$ and $W$ then, $T\supseteq U+W$.
\end{enumerate}
\end{theorem}
Note 1: Let $U$ and $W$ be two vector subspaces of a vector space $V$ over a field $\mathbb{F}$. Then the following are equivalent.
\begin{enumerate}
\item The union $U\cup W$ is a vector subspace of $V$.
\item Either $U\subseteq W$ or $W\subseteq U$.
\end{enumerate}
\begin{definition}[Sum of many subspaces]
\hfill\\
\normalfont Let $V$ be a vector space over a field $\mathbb{F}$ and let $W_i(1\leq i<s)$ be vector subspaces of $V$. The subset
\[
\sum_{i=1}^sW_i\begin{aligned}
&=W_1+\cdots+W_s\\
&=\{\sum_{i=1}^s\mathbf{w}_i\mid \mathbf{w}_i\in W\}\\
&=\{\mathbf{w}_1+\cdots+\mathbf{w}_s\mid \mathbf{w}_i\in W_i\}
\end{aligned}
\]
\end{definition}
\begin{theorem}[Sum of many being a subspace]
\hfill\\
\normalfont Let $W_i (1\leq i<s)$ be vector subspaces of a vector space $V$ over a field $\mathbb{F}$. For the sum $\sum_{i=1}^{s}W_i$, we have
\begin{enumerate}
\item $\sum_{i=1}^{s}W_i = \spn(\cup_{i=1}^sW_i)$
\item $\sum_{i=1}^{s}W_i$ is indeed a vector subspace of $V$.
\item $\sum_{i=1}^{s}W_i$ is the smallest vector subspace of $V$ containing all $W_i$.
\end{enumerate}
\end{theorem}
\begin{definition}[Direct Sum of Subspaces]
\hfill\\
\normalfont Let $V$ be a vector space over a field $\mathbb{F}$ and let $W_1, W_2$ be vector subspace of $V$. We say that the sum $W_1+W_2$ is a \textbf{direct sum} of two vector subspaces $W_1, W_2$ if the intersection
\[
W_1\cap W_2=\{\mathbf{0}\}
\]
In this case, we denote $W_1+W_2$ as $W_1\oplus W_2$.\\
We write $W = W_1\oplus W_2$ if $W$ is a direct sum of $W_1$ and $W_2$.
\end{definition}
\begin{theorem}[Equivalent Direct Sum Definition]
\hfill\\
\normalfont Let $W_1$ and $W_2$ be two vector subspaces of a vector space $V$ over a field $\mathbb{F}$. Set $W:=W_1+W_2$. Then the following are equivalent.
\begin{enumerate}
\item We have
\[
W_1+W_2=W_1\oplus W_2
\]
i.e., $W_1+W_2$ is a direct sum of $W_1,W_2$,i.e., $W_1\cap W_2=\{\mathbf{0}\}$
\item (Unique expression condition) Every vector $\mathbf{w}\in W$ can be expressed as
\[
\mathbf{w}=\mathbf{w}_1+\mathbf{w}_2
\]
for some $\mathbf{w}_i\in W_i$ and such expression of $\mathbf{w}$ is unique.
\end{enumerate}
\end{theorem}
\begin{definition}[Direct Sum of Many Subspaces]
\hfill\\\normalfont Let $V$ be a vector space overa field $\mathbb{F}$ and let $W_i(1\leq i\leq s; s\geq 2)$ be vector subspaces of $V$. We say that the sum $\sum_{i=1}^s W_i$ is a \textbf{direct sum of vector subspaces} $W_i$ if the intersection
\[
\left(\sum_{i=1}^{k-1}W_i\right)\cap W_k =\{\mathbf{0}\}\;\;\;(2\leq \forall k\leq s)
\] 
\end{definition}
\begin{theorem}[Equivalent Direct Multiple Sum Definition]
\hfill\\
\normalfont Let $W_i(1\leq i\leq s, s\geq 2)$ be vector subspaces of a vector space $V$ over a field $\mathbb{F}$. Set $W:=\sum_{i=1}^sW_i$. Then the following are equivalent.
\begin{enumerate}
\item We have
\[
W_1+\cdots+W_s=W_1\oplus\cdots\oplus W_s
\]
i.e., $\sum_{i=1}^sW_i$ is a direct sum of $W_i$.
\item \[
\left(\sum_{i\neq j}W_i\right)\cap W_l=\{\mathbf{0}\}\;\;\;(\forall 1\leq l\leq s)
\]
\item (Unique expression condition) Every vector $\mathbf{w}\in W$ can be expressed as
\[
\mathbf{w}=\mathbf{w}_1+\cdots+\mathbf{w}_s
\]
for some $\mathbf{w}_i\in W_i$ and such expression of $\mathbf{w}$ is unique.
\end{enumerate}
\end{theorem}
\clearpage
\section{Linear Independence, Basis and Dimension}
\begin{definition}[Linear (in)dependence]
\hfill\\\normalfont
Let $V$ be a vector space over a field $\mathbb{F}$. Let $T$ be a (not necessarily finite) subset of $V$ and let
\[
S=\{\mathbf{v}_1,\ldots,\mathbf{v}_n\}
\]
be a finite subset of $V$.
\begin{enumerate}[label=(\arabic*)]
\item We call $S$ a \textbf{linear independent set} or \textbf{L.I.}, if the vector equation below
\[
x_1\mathbf{v}_1+\cdots+x_m\mathbf{v}_m=\mathbf{0}
\]
has only the so called \textbf{trivial solution}
\[
(x_1,\ldots,x_m)=(0,\ldots,0)
\]
\item We call $S$ a \textbf{linear dependent set} or \textbf{L.D.} if there are scalars $a_1,\ldots,a_m$ in $\mathbb{F}$ which are not all zero (i.e. $(a_1,\ldots,a_m)\neq(0,\ldots,0)$) such that
\[
a_1\mathbf{v}_1+\cdots+a_m\mathbf{v}_m=\mathbf{0}
\]
\item The set $T$ is a \textbf{linearly independent set} if every non-empty finite subset of $T$ is linearly independent. The set $T$ is a \textbf{linearly dependent set} if at least one non-empty finite subset of $T$ is linearly dependent.
\end{enumerate}
\end{definition}
\begin{theorem}[L.D./L.I. Inheritance]
\hfill
\normalfont
\begin{enumerate}[label=(\arabic*)]
\item Let $S_1\subseteq S_2$. If the smaller set $S_1$ is linearly dependent then so is the larger set $S_2$. Equivalently, if the larger set $S_2$ is linearly independent then so is the smaller set $S_1$.
\item $\{\mathbf{0}\}$ is a linearly dependent set.
\item If $\mathbf{0}\in S$, then $S$ is a linearly dependent set.
\end{enumerate}
\end{theorem}
\begin{definition}[Equivalent L.I./L.D. Definitions]
\hfill\\
\normalfont Let $S=\{\mathbf{v}_1,\ldots,\mathbf{v}_m\}$ be a finite subset of a vector space $V$ over a field $\mathbb{F}$. Then we have:
\begin{enumerate}[label=(\arabic*)]
\item Let $|S|\geq 2$. Then $S$ is a linear dependent set if and only if some $\mathbf{v}_k\in S$ is a linear combination of the others, i.e. there are scalars
\[
a_1,\ldots,a_{k-1},a_{k+1},\ldots,a_m
\]
in $\mathbb{F}$ (with all these scalars vanishing allowed) such that
\[
\mathbf{v}_k=\sum_{i\neq k}a_i\mathbf{v}_i=a_1\mathbf{v}_1+\cdots+a_{k-1}\mathbf{v}_{k-1}+a_{k+1}\mathbf{v}_{k+1}+\cdots+a_m\mathbf{v}_m
\]
\item Let $|S|\geq 2$. Then $S$ is linearly independent if and only if no $\mathbf{v}_k\in S$ is a linear combination of others.
\item Suppose that $S=\{\mathbf{v}_1\}$ (a single vector). Then $S$ is linearly dependent if and only if $\mathbf{v}_1=\mathbf{0}$. Equivalently, $S$ is linearly independent if and only if $\mathbf{v}_1\neq \mathbf{0}$.
\item Suppose that $S=\{\mathbf{v}_1,\mathbf{v}_2\}$ (two vectors). Then $S$ is linearly dependent if and only if one of $\mathbf{v}_1,\mathbf{v}_2$ is a scalar multiple of the other. Equivalently, $S$ is linearly independent if and only if neither one of $\mathbf{v}_1,\mathbf{v}_2$ is a scalar multiple of the other. 
\end{enumerate}
\end{definition}
\begin{definition}[Basis, (in)finite demension]
\hfill\\\normalfont Let $V$ be a nonzero vector space over a field $\mathbb{F}$. A subset $B$ of $V$ is called a \textbf{basis} if the following two conditions are satisfied.
\begin{enumerate}[label=(\arabic*)]
\item (Span) $V$ is spanned by $B$: $V=\spn(B)$
\item (L.I.) $B$ is a linearly independent set.
\end{enumerate}
If $V$ has a basis $B$ with \textbf{cardinality} $|B|<\infty$ we say that $V$ is \textbf{finite dimensional} and define the \textbf{dimension} of $V$ over the field $\mathbb{F}$ as the cardinality of $B$:
\[
\dim_\mathbb{F}V:=|B|
\]
Otherwise, $V$ is called \textbf{infinite-dimensional}.
\end{definition}
If $V$ equals the zero vector space $\{\mathbf{0}\}$, we define
\[
\dim\{\mathbf{0}\}=0
\]
\begin{theorem}[Equivalent Basis Definition I]
\hfill\\\normalfont Let $B=\{\mathbf{v}_1,\ldots,\mathbf{v}_n\}$(with $\mathbf{v}_i\neq \mathbf{0}_V$) be a finite subset of a vector space $V$ over a field $\mathbb{F}$. Then the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $B$ is a basis of $V$.
\item (Unique expression condition) Every vector $\mathbf{v}\in V$ can be expressed as
\[
\mathbf{v}=a_1\mathbf{v}_1+\cdots+a_n\mathbf{v}_n
\]
for some scalars $a_i\in\mathbb{F}$ and such expression of $\mathbf{v}$ is unique.
\item $V$ has the following direct sum decomposition:
\[V\begin{aligned}
&=\spn\{\mathbf{v}_1\}\oplus\cdots\oplus\spn\{\mathbf{v}_n\}\\
&=\mathbb{F}\mathbf{v}_1\oplus\cdots\oplus\mathbb{F}\mathbf{v}_n
\end{aligned}
\]
\end{enumerate}
\end{theorem}
\begin{theorem}[Deriving a basis from a spanning set]
\hfill\\\normalfont Suppose that a nonzero vector space $V$ over a field $\mathbb{F}$ is spanned by a finite subset $B=\{\mathbf{v}_1,\ldots,\mathbf{v}_s\}$, then we have:
\begin{enumerate}[label=(\arabic*)]
\item There is a subset $B_1\subseteq B$ such that $B_1$ is a basis of $V$. In particular
\[
dim_\mathbb{F}V=B_1\leq B
\]
\item Let $B_2$ be a maximal linearly independent subset of $B$: first $B_2$ is L.I. and secondly every subsets $B_3$ of $B$ larger than $B_2$ is L.D. Then $B_2$ is a basis of $V=\spn(B)$.
\end{enumerate}
\end{theorem}
\begin{theorem}[Dimension being well Defined]
\hfill\\\normalfont Let $B = \{\mathbf{v}_1,\ldots,\mathbf{v}_n\}$ be a basis of a vector space $V$ over a field $\mathbb{F}$. Then we have:
\begin{enumerate}[label=(\arabic*)]
\item Suppose that $S$ is a subset of $V$ with $|S|>n=|B|$. Then $S$ is L.D.
\item Suppose that $T$ is a subset of $V$ with $|T|<n$. Then $T$ does not span $V$.
\item Suppose that $B^\prime$ is another basis of $V$. Then $|B^\prime|=|B|$. So the dimension $\dim_\mathbb{F}V(=|B|)$ of $V$ depends only on $V$, but not on the choice of its basis. \\In other words, $\dim_\mathbb{F}V$ is well defined.
\end{enumerate}
\end{theorem}
\begin{theorem}[Expanding an L.I. set]
\hfill\\\normalfont Let $B$ be a L.I. subset of a vector space $V$ over a field. Then exactly one of the following two cases is true.
\begin{enumerate}[label=(\arabic*)]
\item $B$ spans $V$ and hence $B$ is a basis of $V$.
\item Let $\mathbf{w}\in V\setminus\spn(B)$( and hence $\mathbf{w}\notin B$). Then
\[
B\cup\{\mathbf{w}\}
\]
is a L.I. subset of $V$.
\end{enumerate}
In particular, if $V$ is of finite dimension $n$, then one can find $n-|B|$ vectors 
\[
\mathbf{w}_{|B|+1},\cdots,\mathbf{w}_n
\]
in $V\spn(B)$ such that 
\[
B\coprod\{\mathbf{w}_{|B|+1},\cdots,\mathbf{w}_n\}
\]
is a basis of $V$.
\end{theorem}
\begin{theorem}[Equivalent Basis Definition II]
\hfill\\\normalfont Let $B$ be a subset of vector space $V$ of finite dimension $\dim_mathbb{F}V = n\geq 1$. Then the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $B$ is a basis of $V$.
\item $B$ is L.I. and $|B|=n$.
\item $B$ spans $V$ and $|B|=n$.
\end{enumerate}
\end{theorem}
\begin{theorem}[Basis of a direct sum]
\hfill\\\normalfont Let $V$ be a (not necessarily finite-dimensional) vector space over a field $\mathbb{F}$.
\begin{enumerate}[label=(\arabic*)]
\item Suppose that $B$ is a basis of $V$. Decompose it as a disjoint union
\[
B=B_1\coprod B_2\cdots\coprod B_s
\]
of non-empty sets $B_i$. Then $B_i$ is a basis of $W_i:=\spn(B_i)$ and
\[
V=W_1\oplus\cdots\oplus W_s
\]
is a direct sum of nonzero vector subspaces $W_i$ of $V$.
\item Conversely, suppose that
\[
V=W_1\oplus\cdots\oplus W_s
\]
is a direct sum of nonzero vector subspaces $W_i$ of $V$. Let $B_i$ be a basis of $W_i$. Then
\[
B=B_1\coprod B_2\cdots\coprod B_s
\]
is a basis of $V$ and a disjoint union of non-empty sets $B_i$.
\item In particular, if
\[
V=W_1\oplus\cdots\oplus W_s
\]
is a direct sum, then
\[
\dim_mathbb{F}V=\sum_{i=1}^s\dim_\mathbb{F}W_i
\]
\end{enumerate}
\end{theorem}
\clearpage
\section{Row Space and Column Space}
\begin{definition}[Column/Row Space, Nullspace, Nullity, Range of $A$]
\hfill\\\normalfont
Let
\[
A=(a_ij)=\begin{pmatrix}
a_{11}&\cdots&a_{1n}\\
\vdots&\ddots&\vdots\\
a_{m1}&\cdots&a_{mn}\\
\end{pmatrix}
\]
be an $m\times n$ matrix with entries in a field $\mathbb{F}$. Let
\[
\col(A):=\spn\{\mathbf{c}_1:=\begin{pmatrix}a_{11}\\\vdots\\a_{m1}\end{pmatrix},\ldots,\mathbf{c}_n:=\begin{pmatrix}a_{1n}\\\vdots\\a_{mn}\end{pmatrix}\}
\]
be the \textbf{column space} of $A$, and let
\[
\row(A):=\spn\{\mathbf{r}_1:=(a_{11},\ldots,a_{1n}),\ldots,\mathbf{r}_m:=(a_{m1},\ldots,a_{mn})\}
\]
be the \textbf{row space} of $A$ so that we can write
\[
A=(\mathbf{c}_1,\ldots,\mathbf{c}_m)=\begin{pmatrix}\mathbf{r}_1\\\vdots\\\mathbf{r}_m\end{pmatrix}
\]
The \textbf{range} of $A$ is defined as
\[
R(A)=\{AX\mid X\in\mathbb{F}_c^n\}
\]
The \textbf{nullity} of $A$ is defined as the dimension of the \textbf{nullspace} or \textbf{kernel}
\[
  \kerne(A):=\nul(A):=\{X\in\mathbb{F}_c^n\mid AX=\mathbf{0}\}
\]
i.e. 
\[
\nullity(A)=\dim\nul(A)
\]
\end{definition}
\begin{theorem}[Rank of Matrix, Matrix Dimension Theorem]
\hfill\\\normalfont
\begin{enumerate}[label=(\arabic*)]
\item The range equals the column space
\[
R(A)=\col(A)
\]
\item Column and row spaces have the same dimension
\[
\dim_\mathbb{F}\col(A)=\dim_\mathbb{F}\row(A):=\rank(A)
\]
which is called the \textbf{rank} of $A$.
\item There is a dimension theorem
\[
\rank(A)+\nullity(A)=n
\]
where $n$ is the number of columns in $A$.
\end{enumerate}
\end{theorem}
\begin{theorem}[L.I. vs L.D.]
\hfill\\\normalfont In previous theorem, suppose that $m=n$ so that $A$ is a square matrix of order $n$. Then the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $A$ is an invertible matrix, i.e. $A$ has a so called \textbf{inverse} $A^{-1}\in M_n(\mathbb{F})$ such that
\[
AA^{-1}=I_n=A^{-1}A
\]
\item $A$ has nonzero determinant
\[
\det(A) = |A|\neq 0
\]
\item The column vectors
\[
\mathbf{c}_1,\ldots,\mathbf{c}_n
\]
of $A$ form a basis of the column vector $n$-space $\mathbb{F}_c^n$.
\item The row vectors
\[
\mathbf{r}_1,\ldots,\mathbf{r}_n
\]
of $A$ form a basis of the row vector $n$-space $\mathbb{F}_c^n$.
\item The column vectors
\[
\mathbf{c}_1,\ldots,\mathbf{c}_n
\]
of $A$ are linearly indepedent in $\mathbb{F}_c^n$.
\item The row vectors
\[
\mathbf{r}_1,\ldots,\mathbf{r}_n
\]
of $A$ are linearly indepedent in $\mathbb{F}_c^n$.
\item The matrix equation 
\[
AX=\mathbf{0}
\]
has the trivial solution only: $X=\mathbf{0}$.
\end{enumerate}
\end{theorem}
\begin{theorem}[Row operation preserves columns relations]
\hfill\\\normalfont
Suppose that $A$ and $B$ are row equivalent. Then we have:
\begin{enumerate}[label=(\arabic*)]
\item If the column vectors
\[
\mathbf{a}_{i_1},\ldots,\mathbf{a}_{i_s},\mathbf{a}_{j_1},\ldots,\mathbf{a}_{j_t}
\]
of $A$ satisfies a relation
\[
c_{i_1}\mathbf{a}_{i_1}+\cdots+c_{i_s}\mathbf{a}_{i_s}=c_{j_1}\mathbf{a}_{j_1}+\cdots+c_{j_t}\mathbf{a}_{j_t}
\]
for some scalars $c_{i_k}\in\mathbb{F}$, then the corresponding column vectors
\[
\mathbf{b}_{i_1},\ldots,\mathbf{b}_{i_s},\mathbf{b}_{j_1},\ldots,\mathbf{b}_{j_t}
\]
of $B$ satisfies exactly the same relation
\[
c_{i_1}\mathbf{b}_{i_1}+\cdots+c_{i_s}\mathbf{b}_{i_s}=c_{j_1}\mathbf{b}_{j_1}+\cdots+c_{j_t}\mathbf{b}_{j_t}
\]
The converse is also true.
\item The column vectors
\[
\mathbf{a}_{i_1},\ldots,\mathbf{a}_{i_s},\mathbf{a}_{j_1},\ldots,\mathbf{a}_{j_t}
\]
of $A$ are linearly dependent if and only if the corresponding column vectors
\[
\mathbf{b}_{i_1},\ldots,\mathbf{b}_{i_s},\mathbf{b}_{j_1},\ldots,\mathbf{b}_{j_t}
\]
of $B$ are linearly dependent.
\item The column vectors
\[
\mathbf{a}_{i_1},\ldots,\mathbf{a}_{i_s},\mathbf{a}_{j_1},\ldots,\mathbf{a}_{j_t}
\]
of $A$ are linearly independent if and only if the corresponding column vectors
\[
\mathbf{b}_{i_1},\ldots,\mathbf{b}_{i_s},\mathbf{b}_{j_1},\ldots,\mathbf{b}_{j_t}
\]
of $B$ are linearly independent.
\item The column vectors
\[
\mathbf{a}_{i_1},\ldots,\mathbf{a}_{i_s}
\]
of $A$ forms a basis of the column space
\[
\col(A)=\spn\{\mathbf{a}_1,\ldots,\mathbf{a}_n\}
\] 
if and only if the corresponding column vectors
\[
\mathbf{b}_{i_1},\ldots,\mathbf{b}_{i_s}
\]
form a basis of the column space
\[
\col(B)=\spn\{\mathbf{b}_1,\ldots,\mathbf{b}_n\}
\]
\item If
\[
B_1:=\{\mathbf{a}_{i_1},\ldots,\mathbf{a}_{i_s}\}
\]
is a maximal L.I. subset of the set
\[
C=\{\mathbf{a}_1,\ldots,\mathbf{a}_n\}
\]
of all column vectors then $B_1$ is a basis of the column space $\col(A)$ of $A$.
\item Suppose that $B$ is in row-echelon form with leading entries at columns
\[
i_1,\ldots,i_s
\]
Then
\[
\mathbf{a}_{i_1},\ldots,\mathbf{a}_{i_s}
\]
forms a basis of the column space $\col(A)$ of $A$.
\item The row space of $A$ and $B$ are identical
\[
\row(A)=\row(B)
\]
But the column spaces of $A$ and $B$ may not be the same.
\item Suppose that 
\[
B=(b_{ij})=\begin{pmatrix}\mathbf{b}^\prime_1\\\vdots\\\mathbf{b}^\prime_m\end{pmatrix}
\]
is in row-echelon form with leading entries at rows
\[
j_1,\ldots\j_t
\]
Then
\[
\mathbf{b}^\prime_{j_1},\ldots,\mathbf{b}^\prime_{j_t}
\]
form a basis of the row space $\row(A)=\row(B)$ of $A$.
\end{enumerate}
\end{theorem}
\clearpage
\section{Quotient Spaces and Linear Transformations}
\begin{definition}[Sum of subsets of a space]
\hfill\\\normalfont Let $V$ be a vector space over a field $\mathbb{F}$, and let $S$ and $T$ be subsets (which are not necessarily subspaces) of $V$. Define the \textbf{sum} of $S$ and $T$ as
\[
S+T:=\{s+t\mid s\in S, t\in T\}
\]
In general, given subsets $S_i(1\leq i\leq r)$ of $V$, we can define the \textbf{sum} of $S_i$ as
\[
\sum_{i=1}^r S_i = \{\sum_{i=1}^r \mathbf{x}_i\mid \mathbf{x}_i\in S_i\}
\]
\end{definition}
\begin{theorem}[Inclusion and sum for subsets]
\begin{enumerate}[label=(\arabic*)]
\item Associativity
\[
(S_1+S_2)+S_3= S_1+(S_2+S_3)
\]
\item Commutativity
\[
S_1+S_2=S_2+S_1
\]
\item If $S_1\subseteq S_2$ and $T_1\subseteq T_2$ then
\[
S_1+T_1\subseteq S_2+T_2
\]
\item If $W$ is a subspace of $V$, then
\[
W+\{\mathbf{0}\}=W,\;\;\;\;\;W+W=W
\]
\item Suppose that $W$ is a subspace of $V$, THen
\[
S+W=W\Leftrightarrow S\subseteq W
\]
\end{enumerate}
\end{theorem}
\begin{definition}[Coset $\bar{\mathbf{v}}$]
\hfill\\\normalfont Let $V$ be a vector space over a field $\mathbb{F}$ and $W$ a subspace of $V$. For any given $\mathbf{v}\in V$, the subset
\[
\mathbf{v}+W:=\{\mathbf{v}+\mathbf{w}\mid \mathbf{w}\in W\}
\]
of $V$ is called the \textbf{coset} of $W$ containing $\mathbf{v}$. This subet is often denoted as
\[
\bar{\mathbf{v}}:=\mathbf{v}+W
\]
The vector $\mathbf{v}$ is a \textbf{representative} of the coset $\bar{\mathbf{v}}$.
\end{definition}
\begin{theorem}[Coset Relations]
\hfill\\\normalfont Let $W$ be a subspace of a vector space $V$. The following are equivalent
\begin{enumerate}[label=(\arabic*)]
\item $\mathbf{v}+W=W$,i.e.,$\bar{\mathbf{v}}=\bar{\mathbf{0}}$
\item $\mathbf{v}\in W$
\item $\mathbf{v}+W\subseteq W$
\item $W\subseteq \mathbf{v}+W$
\end{enumerate}
\end{theorem}
\begin{theorem}[To be the same coset]
\hfill\\\normalfont Let $W$ be a subspace of $V$. Then for $\bar{\mathbf{v_i}}=\mathbf{v_i}+W$,
\[
\bar{\mathbf{v_1}}=\bar{\mathbf{v_2}}\Leftrightarrow \mathbf{v_1}-\mathbf{v_2}\in W
\]
\end{theorem}
\textbf{Remark}:Suppose that $V=U\oplus W$ is a direct sum of subspaces $U$ and $W$. Then the map below is a bijection(and indeed an isomorphism)
\[
\begin{aligned}
f:U&\to V/W\\
\mathbf{u}&\mapsto \bar{\mathbf{u}}=\mathbf{u}+W
\end{aligned}
\]
\begin{definition}[Quotient Space]
\hfill\\\normalfont Let $W$ be a subspace of $V$. Let
\[
V/W:=\{\bar{\mathbf{v}}=\mathbf{v}+W\mid\mathbf{v}\in V\}
\]
be the set of all cosets of $W$. It is called the \textbf{quotient space} of $V$ \textbf{modulo} $W$.\\
We define a binary addition operation on $V/W$:
\[
\begin{aligned}
+:V/W\times V/W&\to V/W\\
(\bar{\mathbf{v}_1},\bar{\mathbf{v}_2})&\mapsto \bar{\mathbf{v}_1}+\bar{\mathbf{v}_2}:=\bar{\mathbf{v}_1+\mathbf{v}_2}
\end{aligned}
\]
and a scalar multiplication operation
\[
\begin{aligned}
\times:\mathbb{F}\times V/W&\to V/W\\
(a,\bar{\mathbf{v}_1})&\mapsto a\bar{\mathbf{v}_1}:=\bar{a\mathbf{v}_1}
\end{aligned}
\]
\end{definition}
\begin{theorem}[Quotient Space being Well Defined]
\hfill\\\normalfont Let $V$ be a vector space over a field $\mathbb{F}$ and $W$ a vector subspace of $V$. Then we have:
\begin{enumerate}[label=(\arabic*)]
\item The binary addition operation and scalar multiplication operation on $V/W$ is well defined.
\item $V/W$ together with these binary addition and scalar multiplication operations, becomes a vector space over the same field $\mathbb{F}$, with the zero vector
\[
\mathbf{0}_{V/W}=\bar{\mathbf{0}_V}=\bar{\mathbf{w}}
\]
for any $\mathbf{w}\in W$.
\end{enumerate}
\end{theorem}
\begin{definition}[Linear transformation, and its Kernel and Image; Isomorphism]
\hfill\\\normalfont Let $V_i$ be two vector spaces over the same field $\mathbb{F}$. A map
\[
\varphi:V_1\to V_2
\]
is called a \textbf{linear transformation} from $V_1$ to $V_2$ if $\varphi$ is compatible with the vector addition and scalar multiplication on $V_1$ and $V_2$ in the sense below:
\[
\begin{aligned}
\varphi(\mathbf{v}_1+\mathbf{v}_2)&=\varphi(\mathbf{v}_1)+\varphi(\mathbf{v}_2)\\
\varphi(a\mathbf{v})&=a\varphi(\mathbf{v})
\end{aligned}
\]
When $\varphi:V\to V$ is a linear transformation from $V$ to itself, we call $\varphi$ a linear operator on $V$.\\
A linear transformation is called an \textbf{isomorphism} if it is a bijection. In this case, we denote
\[
V_1\simeq V_2
\]
\end{definition}
\textbf{Remark}: If $T:V\to W$ is a linear transformation, then $T(\mathbf{0}_V)=\mathbf{0}_W$.\\
\textbf{Remark}:(Direct sum vs quotient space)\\
Let $V = U\oplus W$, where $U, W$ are subspaces of $V$. Then the map below is an isomorphism.
\[
\begin{aligned}
f:U&\to V/W\\
\mathbf{u}&\mapsto \bar{\mathbf{u}}=\mathbf{u}+W
\end{aligned}
\]
\begin{theorem}[Equivalent Linear Transformation definition]
\hfill\\\normalfont Let $\varphi:V_1\to V_2$ be a map between two vector spaces $V_i$ over the same field $\mathbb{F}$. The the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $\varphi$ is a linear transformation.
\item $\varphi$ is compatible with taking linear combination in the sense below:
\[
\varphi(a_1\mathbf{v}_1+a_2\mathbf{v}_2)=a_1\varphi(\mathbf{v}_1)+a_2\varphi(\mathbf{v}_2)
\]
for all $a_i\in \mathbb{F},\mathbf{v}_i\in V$.
\end{enumerate}
\end{theorem}
\begin{theorem}[Evaluate $T$ at a basis]
\hfill\\\normalfont Let $V$ be a vector space over a field $\mathbb{F}$ and with a basis $B=\{\mathbf{u}_1,\mathbf{u}_2,\ldots\}$. Let $T:V\to W$ be a linear transformation. \\
Then $T$ is uniquely determined by its valuations $T(\mathbf{u}_i)\;\;\;(i=1,2,\ldots)$ at the basis $B$.\\
Namely, if $T^\prime:V\to W$ is another linear transformation such that $T^\prime(\mathbf{u}_i)=T(\mathbf{u}_i)\forall i$, then they are equal: $T^\prime = T$.
\end{theorem}
\begin{theorem}[Quotient map]
\hfill\\\normalfont Let $V$ be a vector space over a field $\mathbb{F}$. Let $W$ be a subspace $V$ and $V/W$. One verifies that $\gamma$ is surjective and
\[
\ker(\gamma)=W
\]
\end{theorem}
\begin{theorem}[Image being a vector subspace]
\hfill\\\normalfont Let 
\[
\varphi:V\to W
\]
be a linear transformation between two vector spaces over the same field $\mathbb{F}$. Let $V_1$ be a vector subspace of $V$. Then the image of $V_1$:
\[
T(V_1)=\{T(\mathbf{u}\mid \mathbf{u}\in V_1\}
\]
is a vector subspace of $W$.\\
In particular, $T(V)$ is a vector subspace of $W$.
\end{theorem}
\begin{theorem}[Subspace vs. Kernel]
\hfill\\\normalfont Let $V$ be a vector space over a field $\mathbb{F}$.
\begin{enumerate}[label=(\arabic*)]
\item Suppose that
\[
\varphi:V\to U
\]
is a linear transformation. Then the \textit{kernel} $\ker(\varphi)$ is a vector subspace of $V$.
\item Conversely, suppose $W$ is a vector subspace of $V$. Then there is a linear transformation
\[
\varphi:V\to U
\]
such that
\[
W=\ker(\varphi)
\]
\end{enumerate}
\end{theorem}
\begin{theorem}[To be injective]
Let
\[
\varphi:V\to W
\]
be a linear transformation. Show that $\varphi$ is injective if and only if $\ker(\varphi)=\{\mathbf{0}\}$.
\end{theorem}
\begin{theorem}[Equivalent Isomorphism Definition]
\hfill\\\normalfont Let $\varphi:V\to W$ be a linear transformation. Then there is an isomorphism
\[
\begin{aligned}
\bar{\varphi}:V/\ker(\varphi)&\simeq \varphi(V)\in U\\
\bar{\mathbf{v}}&\mapsto\varphi(\mathbf{v})
\end{aligned}
\]
such that
\[
\varphi=\bar{\varphi}\circ\gamma
\]
where
\[
\begin{aligned}
\gamma:V&\to V/\ker(\varphi)
\mathbf{v}&\mapsto\bar{\mathbf{v}}
\end{aligned}
\]
is the quotient map, a linear transformation.\\
In particular, when $\varphi$ is surjective, we have an isomorphism
\[
\bar{\varphi}:V/\ker(\varphi)\simeq U
\]
\end{theorem}
\begin{theorem}[Finding basis of the quotient]
\hfill\\\normalfont
Let $V$ be a vector space over a field $\mathbb{F}$ of finite dimension $n$. Let $W$ be a subspace with a basis $B_1=\{\mathbf{w}_1,\ldots,\mathbf{w}_r\}$.
\begin{enumerate}[label=(\arabic*)]
\item $B_1$ extends to a basis
\[
B:=B_1\coprod\{\mathbf{w}_1,\ldots,\mathbf{w}_r\}
\]
of $V$.
\item The cosets
\[
\{\bar{\mathbf{w}_1},\ldots,\bar{\mathbf{w}_r}\}
\]
is a basis of the quotient space $V/W$. In particular,
\[
\dim_\mathbb{F}V/W=\dim_\mathbb{F}V-\dim_\mathbb{F}W
\]
\item 
\[
B_1\coprod\{\mathbf{u}_{r+1},\ldots,\mathbf{u}_n\}
\]
is a basis of $V$ if and only if the cosets
\[
\{\bar{\mathbf{u}_{r+1}},\ldots,\bar{\mathbf{u}_n}\}
\]
is a basis of $V/W$.
\end{enumerate}
\end{theorem}
\begin{theorem}[Goodies of Isomorphism]\hfill\\\normalfont
Let $\varphi:V\to W$ be an isomorphism and let $B$ be a subset of $V$. Then we have:
\begin{enumerate}[label=(\arabic*)]
\item If there is a relation
\[
\sum_{i=1}^r a_i\mathbf{v}_i = \sum_{i=r+1}^sa_i \mathbf{v}_i
\]
among vectors $\mathbf{v}_i\in V$, then exactly the same relation
\[
\sum_{i=1}^r a_i\varphi(\mathbf{v}_i) = \sum_{i=r+1}^sa_i \varphi(\mathbf{v}_i)
\]
holds among vectors $\varphi(\mathbf{v}_i)\in W$. The converse is also true.
\item $B$ is linearly dependent if and only if so is $\varphi(B)$.
\item $B$ is linearly independent if and only if so is $\varphi(B)$;
\item We have 
\[
\varphi(\spn(B))=\spn(\varphi(B))
\]
In particular, 
\[
\varphi(\spn\{\mathbf{v}_1,\ldots,\mathbf{v}_s\})=\spn\{\varphi(\mathbf{v}_1),\ldots, \varphi(\mathbf{v}_s)\}
\]
\item $B$ spans $V$ if and only if $\varphi(B)$ spans $W$.
\item $B$ is a basis of $V$ if and only if $\varphi(B)$ is a basis of $W$. In particular,
\[
\dim V = \dim W
\]
\end{enumerate}
\end{theorem}
\begin{theorem}[To be isomorphic finite-dimensional spaces]
\hfill\\\normalfont Let $V$ and $W$ be finite-dimensional vector spaces over the same field $\mathbb{F}$. Then the following are equivalent.
\begin{enumerate}[label = (\arabic*)]
\item $\dim_\mathbb{F}V=\dim_\mathbb{F}W=n$.
\item There is an isomorphism
\[
\varphi:V\simeq W
\]
\item For some $n$, we have:
\[
V\simeq \mathbb{F}^n\simeq W
\]
\end{enumerate}
\end{theorem}
\begin{theorem}[Dimension Theorem]
\hfill\\\normalfont Let $\varphi: V\to W$ be a linear transformation between vector spaces over a field $\mathbb{F}$. Then
\[
\dim_\mathbb{F}\ker(\varphi)+\dim_\mathbb{F}\varphi(V)=\dim_\mathbb{F}V
\]
\end{theorem}
\begin{theorem}[2nd Isomorphism Theorem]
\hfill\\\normalfont Let $W_1,W_2$ be vector subspaces of a vector space $V$.
\begin{enumerate}[label=(\arabic*)]
\item The map
\[
\begin{aligned}
\varphi:W+1/(W_1+W_2))&\to(W_1+W_2)/W_2\\
\mathbf{w}+W_1\cap W_2=\overline{\mathbf{w}}\mapsto\overline(\mathbf{w})=\mathbf{w}+W_2
\end{aligned}
\]
is a well difined isomorphism between vector spaces.
\item A dimension formula:
\[
\dim W_1 + \dim W_2 = \dim(W_1+W_2)+\dim(W_1\cap W_2)
\]
\end{enumerate}
\end{theorem}
\begin{theorem}[Equivalent isomorphism definition]
\hfill\\\normalfont Let
\[
\varphi:V\to W
\]
be a linear transformation between vector spaces over a field $\mathbb{F}$ and of the same finite dimension $n$. Then the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $\varphi$ is an isomorphism
\item $\varphi$ is an injection
\item $\varphi$ is an surjection
\end{enumerate}
\end{theorem}
\clearpage
\section{Representation Matrices of Linear Transformations}
\begin{definition}[Coordinate vector]
\hfill\\\normalfont Let $V$ be vector space of dimension $n\geq 1$ over a field $\mathbb{F}$. Let
\[
B=B_V=(\mathbf{v}_1,\ldots,\mathbf{v}_n)
\]
be a basis of $V$. Every vector $\mathbf{v}\in V$ can be expressed as a linear combination
\[
\mathbf{v}=c_1\mathbf{v}_1+\cdots+c_n\mathbf{v}_n
\]
and this expression is unique. We gather the coefficients $c_i$ and form a column vector
\[
[\mathbf{v}]_B:=\begin{pmatrix}c_1\\\vdots\\c_n\end{pmatrix}\in\mathbb{F}_c^n
\]
which is called the \textbf{coordinate vector} of $\mathbf{v}$ related to basis $B$.\\
One can recover $\mathbf{v}$ from its coordinate vector $[\mathbf{v}]_B$:
\[
\mathbf{v}=B[\mathbf{v}]_B
\]
\end{definition}
\begin{theorem}[Isomorphism $V\to \mathbb{F}_c^n$]
\hfill\\\normalfont Let $V$ be an $n$-dimensional vector space over a field $\mathbb{F}$ and with a basis $B=\{\mathbf{v}_1,\ldots,\mathbf{v}_n\}$. Show that the map
\[
\begin{aligned}
\varphi:V&\to\mathbb{F}_c^n\\
\mathbf{v}&\mapsto[\mathbf{v}]_B
\end{aligned}
\]
is an isomorphism between the vector space $V$ and $\mathbb{F}_c^n$.
\end{theorem}
\begin{theorem}[Representation matrix]
\hfill\\\normalfont Let 
\[
T:V\to W
\]
be a linear transformation between vector spaces over a field $\mathbb{F}$. Let
\[
B=\{\mathbf{v}_1,\ldots,\mathbf{v}_n\}
\]
be a basis of $V$, and
\[
B_W=\{\mathbf{w}_1,\ldots,\mathbf{w}_m\}
\]
be a basis of $W$. Let $A\in \mathbb{M}_{m\times n}(\mathbb{F})$. Then the following three conditions on $A$ are equivalent. 
\[
[T(\mathbf{v})]_{B_W}=A[\mathbf{v}]_B
\]
\[
A=([T(\mathbf{v}_1)]_{B_W},\ldots,[T(\mathbf{v}_n)]_{B_W})
\]
\[
(T(\mathbf{v}_1),\ldots,T(\mathbf{v}_n))=(\mathbf{w}_1,\ldots,\mathbf{w}_m)A
\]
We denote the above matrix $A$ as
\[
[T]_{B,B_W}:=A=([T(\mathbf{v}_1)]_{B_W},\ldots,[T(\mathbf{v}_n)]_{B_W})
\]
and call it the \textbf{representation matrix} of $T$ relative to $B$ and $B_W$.
\end{theorem}
\begin{theorem}[Linear transformation theory = Matrix theory]
\hfill\\\normalfont For every matrix
\[
A\in\mathbb{M}_{m\times n}(\mathbb{F})
\]
there is a unique linear transformation
\[
T:V\to W
\]
such that the representation matrix
\[
[T]_{B,B_W}=A
\]
Consequently, the map
\[
\begin{aligned}
\varphi:\Hom_\mathbb{F}(V,W)&\to\mathbb{M}_{m\times n}(\mathbb{F})\\
T&\mapsto [T]_{B,B_W}
\end{aligned}
\]
is an isomorphism of vector spaces over $\mathbb{F}$.
\end{theorem}
\begin{theorem}[Close relation between the space of vectors and space of their coordinates]
\hfill\\\normalfont Let 
\[
T:V\to W
\]
be a linear transformation between the vector spaces $V$ and $W$ over the same field $\mathbb{F}$, of dimensions $n$ and $m$, respectively. Let
\[
B_V:=(\mathbf{v}_1,\ldots,\mathbf{v}_n)
\]
be a basis of $V$, and
\[
B_W:=(\mathbf{w}_1,\ldots,\mathbf{w}_m)
\]
be a basis of $W$. Let
\[
M_{m\times n}(\mathbb{F})\ni A:=[T]_{B_V,B_W} = (\mathbf{a}_1,\ldots,\mathbf{a}_n)
\]
Then the following are isomorphisms:
\[
\begin{aligned}
\varphi: \kerne(T)&\to\nul(A)\\
\mathbf{v}&\mapsto[\mathbf{v}]_{B_V},\\
\psi:\nul(A)&\to\kerne(T)\\
X&\mapsto B_V X,\\
\xi:\row(T)&\to \row(T_A)=\text{col.sp. of }A = \spn\{\mathbf{a}_1,\ldots,\mathbf{a}_n\}\\
\eta:\row(T_A)&\to\row(T)\\
Y&\mapsto B_WY=(\mathbf{w}_1,\ldots,\mathbf{w}_m)Y
\end{aligned}
\]
Below are some consequences of the isomorphism above
\begin{enumerate}[label=(\arabic*)]
\item The subset
\[
\{X_1,\ldots,X_s\}
\]
of $\mathbb{F}^n_c$ is a basis of $\nul(A)$ if and only if the vectors
\[
B_VX_1,\ldots,B_VX_S
\]
of $V$ forms a basis of $\kerne(T)$.
\item The subset 
\[
\{Y_1,\ldots,Y_t\}
\]
of $\mathbb{F}^m_c$ is a basis of $\row(T_A)$ if and only if the vectors
\[
B_WY_1,\ldots,B_WY_t
\]
of $W$ form a basis of $\row(T)$.
\item The range of $T$ is given by
\[
\row(T)=\spn\{B_W\mathbf{a}_1,\ldots,B_W\mathbf{a}_n\}
\]
\item $T:V\to W$ is an isomorphism if and only if its representation matrix $A = [T]_{B_V,B_W}$ is an invertible matrix in $M_n(\mathbb{F})$.
\end{enumerate}
\end{theorem}
\begin{theorem}[Representation Matrix of a Composite Map]
\hfill\\\normalfont Let $V_1, V_2,V_3$ be vector spaces of finite dimension over the same field $\mathbb{F}$ and let $B_1, B_2, B_3$ be theire respective bases. Let
\[
T_1:V_1\to V_2
\]
and
\[
T_2:V_2\to V_3
\]
be linear transformations. Then we have:
\[
[T_2\circ T_1]_{B_1,B_3}=[T_2]_{B_2,B_3}[T_1]_{B_1,B_2}
\]
\end{theorem}
\begin{theorem}[Representation matrix of inverse of an isomorphism]
\hfill\\\normalfont Let 
\[
T:V\to W
\]
be an isomorphism between vector spaces over the same field $\mathbb{F}$ and of finite dimension. Let
\[
T^{-1}:W\to V
\]
be the inverse isomorphism of $T$. Let $B_V$(resp. $B_W$) be a basis of $V$(resp. $W$). Then
\[
[T^{-1}]_{B_W,B_V} = [T_{B_V,B_W}]^{-1}
\]
\end{theorem}
\begin{theorem}[Representation matrix of map combination]
\hfill\\\normalfont Let
\[
T_i:V\to W
\]
be two linear transformations between finite-dimensional vector spaces over the same field $\mathbb{F}$. Let $B$(resp. $B_W$) be a basis of $V$(resp. $W$). Then for any $a_i\in\mathbb{F}$, the map linear combination $a_1T_1+a_2T_2$ has the representation matrix
\[
[a_1T_1+a_2T_2]_{B,B_W} = a_1[T_1]_{B,B_W}+a_2[T_2]_{B,B_W}
\]
\end{theorem}
\begin{theorem}[Equivalent transition matrix definition]
\hfill\\\normalfont Let $V$ be a vector space over a field $\mathbb{F}$ and of finite dimension $n\geq 1$. Let
\[
B=(\mathbf{v}_1,\ldots, \mathbf{v}_n)
\]
and
\[
B^\prime:=(\mathbf{v}^\prime_1,\ldots,\mathbf{v}^\prime_n)
\]
be two bases of $V$. Let $P\in M_n(\mathbb{F})$. Then the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item 
\[
P=([\mathbf{v}_1^\prime]_B, \ldots, [\mathbf{v}_n^\prime]_B)
\]
\item
\[
B^\prime = BP
\]
\item For any $\mathbf{v}\in V$, we have
\[
P[\mathbf{v}]_{B^\prime} = [\mathbf{v}]_B
\]
\end{enumerate}
This $P$ is denoted as $P_{B^\prime\to B}$ and called the \textbf{transition matrix} from basis $B^\prime$ to $B$. $P$ is invertible.  
\end{theorem}
\begin{theorem}[Basis change theorem for representation matrix]
\hfill\\\normalfont Let $V$ be a vector space over a field $\mathbb{F}$ and of finite dimension $n\geq 1$. Let
\[
B=(\mathbf{v}_1,\ldots,\mathbf{v}_n)
\]
and
\[
B^\prime:=(\mathbf{v}^\prime_1,\ldots,\mathbf{v}^\prime_n)
\]
be two bases of $V$. Then
\[
[T]_{B^\prime} = P^{-1}[T]_BP
\]
where
\[
P=P_{B^\prime\to B}
\]
\end{theorem}
\begin{definition}[Similar Matrices]
\hfill\\\normalfont Two square matrices (of the same order) $A_1, A_2\in\mathbb{M}_n(\mathbb{F})$ are \textbf{similar} if there is an invertible matrix $P\in\mathbb{M}_n(\mathbb{F})$ such that
\[
A_2 =P^{-1}A_1P
\]
In this case, we denote
\[
A_1\sim A_2
\]
The similarity property is an equivalence relation.
\end{definition}
\begin{theorem}\normalfont Similar matrices have the same determinant:
\[
A_1\sim A_2\Rightarrow |A_1|=|A_2|
\]
\end{theorem}
\begin{definition}[Determinant/Trace of a linear operator]
\hfill\\\normalfont Let
\[
T:V\to V
\]
be a linear operator on a finite-dimensional vector space $V$.\\
We define the \textbf{determinant} $\det(T)$ of $T$ as 
\[
\det(T):=\det([T]_B)
\]
and the \textbf{trace} of $T$ as
\[
\Tr(T)=\Tr([T]_B)
\]
where $B$ is any basis of $V$.
\end{definition}
\begin{definition}[Characteristic polynomial $p_A(x),p_T(x)$]
\normalfont
\begin{enumerate}[label=(\arabic*)]
\item Let $A\in\mathbb{M}_n(\mathbb{F})$.
\[
\begin{aligned}
p_A(x):&=|xI_n-A|\\
&=x^n+b_{n-1}x^{n-1}+\cdots+b_1x+b_0
\end{aligned}
\]
is called the \textbf{characteristic polynomial} of $A$, which is of degree $n$.
\item Let 
\[
T:V\to V
\]
be a linear operator on an $n$-dimensional vector space $V$. Set
\[
A:=[T]_B
\]
where $B$ is any basis of $V$. Then
\[
\begin{aligned}
p_T(x):&=|xI_n-A|\\
&=x^n+b_{n-1}x^{n-1}+\cdots+b_1x+b_0
\end{aligned}
\]
is called the \textbf{characteristic polynomial} of $T$, which is of degree $n=\dim V$.
\end{enumerate}
\end{definition}
\begin{theorem}\normalfont Similar matrices have equal characteristic polynomial.
\end{theorem}
\begin{theorem}
\hfill\\\normalfont For $A\in\mathbb{M}_n(\mathbb{F})$, we have
\[
\Tr(A)=-b_{n-1}
\]
\[
\det(A)=(-1)^np_A(0)
\]
\end{theorem}
\clearpage
\section{Eigenvalue and Cayley-Hamilton Theorem}
\begin{definition}[Eigenvalue, eigenvector]
\hfill\\\normalfont Assume that
\[
\lambda\in\mathbb{F}
\]
\begin{enumerate}[label=(arabic*)]
\item Let $V$ be a vector space over a field $\mathbb{F}$. Let
\[
T:V\to V
\]
be a linear operator. A \textit{nonzero} vector $\mathbf{v}$ in $V$ is called an \textbf{eigenvector} of $T$ corresponding to the eigenvalue $\lambda\in\mathbb{F}$ of $T$ if
\[
T(\mathbf{v})=\lambda\mathbf{v}
\]
\item For an $n\times n$ matrix $A$ in $\mathbb{M}_n(\mathbb{F})$, a nonzero column vector $\mathbf{u}$ in $\mathbb{F}^n_c$ is called an \textbf{eigenvector} of $A$ corresponding to the eigenvalue $\lambda\in\mathbb{F}$ of $A$ if
\[
A\mathbf{u}=\lambda\mathbf{u}
\]
\end{enumerate}
\end{definition}
\begin{definition}[Equivalent definition of eigenvalue and eigenvector]
\hfill\\\normalfont Let $V$ be a vector space of dimension $n$ over a field $\mathbb{F}$ and with a basis $B$, Let
\[
T:V\to V
\]
be a linear operator. Assume that
\[
\lambda\in\mathbb{F}
\]
Then the following are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item $\lambda$ is an eigenvalue of $T$ (corresponding to an eigenvector $\mathbf{0}\neq\mathbf{v}\in V$ of $T$, i.e. $T(\mathbf{v})=\lambda \mathbf{v}$).
\item $\lambda$ is an eigenvalue of $[T]_B$ (corresponding to an eigenvector $\mathbf{0}\neq[\mathbf{v}]_B\in\mathbb{F}^n_c$ of $[T]_B$, i.e. $[T]_B[v]_B =\lambda[\mathbf{v}]_B$).
\item The linear operator
\[
\begin{aligned}
\lambda I_V-T:V&\to V\\
\mathbf{x}&\mapsto \lambda \mathbf{x}-T(\mathbf{x})
\end{aligned}
\]
is not an isomorphism, i.e. there is some 
\[
0\neq \mathbf{v}\in\kerne(\lambda I_V-T)
\]
\item The matrix $\lambda I_n-[T]_B$ is not invertiblem i.e. the matrix equation
\[
(\lambda I_n-[T]_B)X=0
\]
has a non=trivial solution.
\item $\lambda$ is a zero of the characteristic polynomial $p_T(x)$ of $T$
\[
p_T(\lambda)=|\lambda I_n-[T]_B|=0
\]
\end{enumerate}
\end{definition}
\begin{theorem}[Determinant $|A|$ as product of eigenvalues]
\hfill\\\normalfont Let $A\in\mathbb{M}_n(\mathbb{F})$. Let $p(x)$ be the characteristic polynomial. Factorise
\[
p(x)=(x-\lambda_1)\cdots(x-\lambda_n)
\]
in some over field of $\mathbb{F}$. Then the determinant of $A$ equals
\[
\prod_{i=1}^n\lambda_i
\]
\end{theorem}
\begin{definition}[Eigenspace of an eigenvalue]
\hfill\\\normalfont Let $\lambda\in\mathbb{F}$ be an eigenvalue of a linear operator
\[
T:V\to V
\]
on an $n$-dimensional vector space $V$ over the field $\mathbb{F}$.
The subspace (of all the eigenvectors corresponding to the eigenvalue $\lambda$, plus $\mathbf{0}_V$):
\[
\begin{aligned}
V_\lambda:&=V_\lambda(T)\\
:&=\kerne(\lambda I_V-T)\\
&=\{\mathbf{v}\in V\mid T(\mathbf{v})=\lambda\mathbf{v}\}
\end{aligned}
\]
of $V$ is called the \textbf{eigenspace} of $T$ corresponding to the eigenvalue $\lambda$.
\end{definition}
\begin{definition}[Geometric/Algebraic Multiplicity]
\hfill\\\normalfont Let $\lambda\in\mathbb{F}$ and $T:V\to V$ be as the previous definition.
\begin{enumerate}[label=(\arabic*)]
\item The dimension
\[
\dim V_\lambda
\]
of the eigenspace $V_\lambda$ of $T$ is called the \textbf{geometrix multiplicity} of the eigenvalue $\lambda$ of $T$. We have
\[
1\leq \dim V_\lambda\leq n
\]
\item The \textbf{algebraic multiplicity} of the eigenvalue $\lambda$ of $T$ is defined to be the largest positive integer $k$ such that $(x-\lambda)^k$ is a \textbf{factor} of the characteristic polynomial $p_T(x)$,i.e.
\[
(x-\lambda)^k\mid p_T(x),\;\;(x-\lambda)^{k+1}\nmid p_T(x)
\] 
\end{enumerate}
We shall see that
\[
\text{geometric multiplicity of }\lambda\leq \text{alg. multiplicity of }\lambda
\]
\end{definition}
\begin{theorem}[{Eigenspace of $T$ and $[T]_B$}]
\hfill\\\normalfont
Let
\[
T:V\to V
\]
be a linear operator on an $n$-dimensional vector space $V$ with a basis $B$. Set
\[
A:=[T]_B
\]
The map
\[
\begin{aligned}
f:\kerne(T-\lambda I_V)&\to\nul(A-\lambda I_n)\\
\mathbf{w}&\mapsto[\mathbf{w}]_B
\end{aligned}
\]
gives an isomorphism. In particular,
\[
\dim V_lambda(T)=\dim V_\lambda(A)
\]
Due to this isomorphism, the following are equivalent:
\begin{enumerate}
\item The subset
\[
\{\mathbf{u}_1,\cdots, \mathbf{u}_s\}
\]
of $V$ is a basis of the eigenspace $V_\lambda(T)$ of $T$.
\item THe subset
\[
\{[\mathbf{u}_1]_B,cdots,[\mathbf{u}_s]_B\}
\]
of $\mathbb{F}_c^n$ is a basis of the eigenspace $V_\lambda([T]_B)$ of the representation matrix $[T]_B$ of $T$ relative to a basis $B$ of $V$.
\end{enumerate}
Also, the following are equivalent:
\begin{enumerate}
\item The subset
\[
\{X_1,\cdots, X_s\}
\]
of $\mathbb{F}^n_c$ is a basis of the eigenspace $V_\lambda([T]_B)$ of the representation matrix $[T]_B$ of $T$ relative to a basis $B$ of $V$.
\item The subset
\[
\{BX_1, \cdots,BX_s\}
\]
of $V$ is a basis of the eigenspace $V_\lambda(T)$ of $T$.
\end{enumerate}
\end{theorem}
\begin{theorem}[Eigenspaces of similar matrices]
\hfill\\\normalfont Let $A\in\mathbb{M}_n(\mathbb{F})$. Suppose that $P^{-1}AP=C$. Show that
\[
\mathbb{F}^n_c\supseteq V_\lambda(A)=PV_\lambda(C):=\{PX\mid X\in V_\lambda(C)\}
\]
\end{theorem}
\begin{theorem}[Sum of eigenspaces]
\hfill\\\normalfont Let
\[
\lambda_1,\ldots,\lambda_k
\]
be some distinct eigenvalues of a linear operator $T$ on a vector space $V$ over a field $\mathbb{F}$. Then the sum of eigenspaces
\[
\begin{aligned}
W:&=\sum_{i=1}^kV_{\lambda_i}(T)\\
&=V_{\lambda_i}(T)+\cdots+V_{\lambda_k}(T)
\end{aligned}
\]
is a direct sum:
\[
\begin{aligned}
W&=\oplus_{i=1}^k V_{\lambda_i}(T)\\
&=V_{\lambda_1}(T)\oplus\cdots\oplus V_{\lambda_k}(T)
\end{aligned}
\]
\end{theorem}
\begin{definition}[Multiplication of linear operators $S_1,\ldots,S_r$]
\hfill\\\normalfont Let
\[
T:V\to V
\]
be a linear operator on a vector space $V$ over a field $\mathbb{F}$. Define
\[
T^s:=T\circ\cdots\circ T\;\;\;(s \text{ times})
\]
which is a linear operator on $V$.
\[
\begin{aligned}
T^s:V&\to V\\
\mathbf{v}&\mapsto T^s(\mathbf{v})
\end{aligned}
\]
Be convention, set
\[
T^0:=I_V=\id_V
\]
More generally, for a polynomial
\[
f(x)=\sum_{i=0}^r a_ix^i
\]
Define
\[
f(T)=\sum_{i=0}^r a_iT^i
\]
Then $f(T)$ is a linear operator on $V$.
\[
\begin{aligned}
f(T):V\&to V\\
\mathbf{v}&\mapsto f(T)(\mathbf{v})
\end{aligned}
\]
Similarly, for linear operators
\[
S_i:V\to V
\]
Define
\[
S_1S_2\cdots S_r:=S_1\circ S_2\circ\cdots \circ S_r
\]
which is a linear operator.
\end{definition}
\begin{theorem}[Polynomials in $T$]
\hfill\\\normalfont Let
\[
T:V\to V
\]
be a linear operator on a vector space $V$ over a field $\mathbb{F}$ and with a basis
\[
B=\{\mathbf{u}_1,\ldots,\mathbf{u}_n\}
\]
Let
\[
f(x),g(x)\in \mathbf{F}[x]
\]
be polynomials. We have
\begin{enumerate}[label=(\arabic*)]
\item 
\[
[f(T)]_B = f([T]_B)
\]
\item The multiplication $f(T)g(T)$ as polynomials in $T$ equals the composite $f(T)\circ g(T)$ as linear operators:
\[
f(T)g(T)=f(T)\circ g(T)
\]
\item Commutativity:
\[
f(T)g(T)=g(T)f(T)
\]
\item If $P\in\mathbb{M}_n(\mathbb{F})$ is invertible, then
\[
f(P^{-1}AP)=P^{-1}f(A)P
\]
\item If
\[
S:V\to V
\]
is an isomorphism with inverse isomorphism
\[
S^{-1}:V\to V
\]
Then
\[
f(S^{-1}TS)=S^{-1}f(T)S
\]
\end{enumerate}
\end{theorem}
\begin{definition}[$T$-invariant Subspace]
\hfill\\\normalfont Let
\[
T:V\to V
\]
be a linear operator on a vector space $V$. A subspace $W$ of $V$ is called $T$-\textbf{invariant} if the image of $W$ under the map $T$ is included in $W$:
\[
T(W):=\{T(\mathbf{w})\mid \mathbf{w}\in W\}
\]
i.e.,
\[
T(\mathbf{w})\in W \;\;\forall \mathbf{w}\in W
\]
In this case, define the \textbf{restriction} of $T$ on $W$ as:
\[
\begin{aligned}
T\mid W:W&\to W\\
\mathbf{w}&\mapsto T(\mathbf{w})
\end{aligned}
\] 
\end{definition}
\begin{theorem}[Kernels and Images of Commutative Operators]
\hfill\\\normalfont Let $T_i:V\to V$ be two linear operator commutative to each other, i.e.
\[
T_1\circ T_2 = T_2\circ T_1
\]
as maps. Namely,
\[
T_1(T_2(\mathbf{v}))=T_2(T_1(\mathbf{v}))\;\;\;(\forall \mathbf{v}\in V)
\]
Both
\[
\kerne(T_2),\;\;\;\\ima(T_2)
\]
are $T_1$-invariant subspaces of $V$.
\end{theorem}
\begin{theorem}[Evaluate $T$ on a basis of a subspace]
\hfill\\\normalfont 
Let
\[
T:V\to V
\] 
be a linear operator on a vector space $V$ over a field $\mathbb{F}$. Let $W$ be a subspace of $V$ with a basis $B_W = \{\mathbf{w}_1,\mathbf{w}_2,\cdots\}$. Then $W$ is $T$-invariant, if and only if
\[
T(B_W)\subseteq W
\]
\end{theorem}
\begin{theorem}[$T$-cyclic subspace]
\hfill\\\normalfont Let
\[
T:V\to V
\]
be a linear operator on a vector space $V$ over a field $\mathbb{F}$. Fix a vector
\[
\mathbf{0}\neq \mathbf{w}_1\in V
\]
\begin{enumerate}
\item The subspace
\[
W:=\spn\{T^s(\mathbf{w}_1\mid s\geq 0\}
\]
of $V$ is $T$-invariant.\\
$W$ is called the $T$-cyclic subspace of $V$ generated by $\mathbf{w}_1$.
\item Suppose that $V$ is finite-dimensional. Let $s$ be the smallest positive integer such that
\[
T^s(\mathbf{w}_1)\in \spn\{\mathbf{w}_1, T(\mathbf{w}_1),\ldots, T^{s-1}(\mathbf{w}_1)\}
\]
We have
\[
\dim_\mathbb{F}W=s
\]
and
\[
B:=\{\mathbf{w}_1, T(\mathbf{w}_1),\ldots, T^{s-1}(\mathbf{w}_1)\}
\]
is a basis of $W$.
\item In (2), if
\[
T^s(\mathbf{w}_1)=c_0\mathbf{w}_1+c_1 T(\mathbf{w}_1)+\cdots+c_{s-1} T^{s-1}(\mathbf{w}_1)
\]
for some scalars $c_i\in \mathbb{F}$, then the characteristic polynomial of the restiction operator $T\mid W$ on $W$ is
\[
p_{T\mid W}(x)=-c_0-c_1x-\cdots-c_{s-1}x^{s-1}+x^s
\]
\end{enumerate}
\end{theorem}
\begin{theorem}[Characteristic Polynomial of the Restriction Operator]
\hfill\\\normalfont
Let
\[
T:V\to V
\]
be a linear operator on a vector space $V$ over a field $\mathbb{F}$ and of dimension $n\geq 1$. Let $W$ be a $T$-invariant subspace of $V$. Then the characteristic polynomial $p_{T\mid W}(x)$ of the restriction operator $T\mid W$ on $W$ is a factor of the characteristic polynomial $p_T(x)$ of $T$, i.e.
\[
p_T(x)=q(x)p_{T\mid W}(x)
\]
for some polynomial $q(x)\in \mathbb{F}[x]$.
\end{theorem}
\begin{theorem}[{To be $T$-invariant in terms of $[T]_B$}]
\hfill\\\normalfont A subspace $W$ of an $n$-dimensional space $V$ is $T$-invariant for a linear operator $T$ on $V$, if and only if every basis $B_W$ of $W$ can be extended to a basis
\[
B=B_W\cup B_2
\]
of $V$ such that the representation matrix of $T$ relative to $B$, is of the form:
\[
[T]_B=\begin{pmatrix}
A_1&A_2\\
0&A_3
\end{pmatrix}
\]
for some square matrices $A_1, A_3$ (automatically with $A_1=[T\mid W]_{B_W}$).\\
In this case, the matrix
\[
A_2=0
\]
if and only if
\[
W_2:=\spn(B_2)
\]
is a $T$-invariant subspace of $V$ (automatically with $[T\mid W_2]_{B_2}=A_3$)
\end{theorem}
\begin{theorem}[Upper Triangular Form of a Matrix]
\hfill\\\normalfont Let $T:V\to V$ be a linear operator on an $n$-dimensional vector space $V$ over a field $\mathbb{F}$. Suppose that the characteristic polynomial $p(x)$ is factorised as
\[
p(x)=(x-\lambda_1)^{n_1}\cdots(x-\lambda_k)^{n_k}
\]
for some $\lambda_i\in\mathbb{F}$. Then there is an basis $B$ of $V$ such that the representation matrix $[T]_B$ is upper triangular.
\end{theorem}
\begin{theorem}[Characteristic Polynomials of Direct Sums]
\hfill\\\normalfont Let 
\[
T:V\to V
\]
be a linear operator on an $n$-dimensional vector space $V$ over a field $\mathbb{F}$. Suppose that there are $T$-invariant subspaces
\[
W_i\;\;\;(1\leq i\leq r)
\]
of $V$ such that $V$ is the direct sum
\[
V=\oplus_{i=1}^r W_i
\]
of $W_i$.\\
Then the characteristic polynomial $p_T(x)$ of $T$ is the product:
\[
p_T(x)=\prod_{i=1}^r p_{T\mid W_i}(x)
\]
of the characteristic polynomials of the restriction operators $T\mid W_i$ on $W_i$.
\end{theorem}
\begin{theorem}[To be direct sum of $T$-invariant subspaces]
\hfill\\\normalfont An $n$-dimensional vector space $V$ with a linear operator $T$ is a direct sum
\[
V=\oplus_{i=1}^r W_i
\]
of some $T$-invariant subspaces $W_i$, if and only if every set of bases $B_i$ of $W_i$ gives rises to a basis
\[
B=B_1\coprod\cdots\coprod B_r
\]
of $V$ such that the representation matrix of $T$ relative to $B$ is in the form
\[
[T]_B = \begin{pmatrix}
A_1&0&\cdots&0\\
0&A_2&\cdots&0\\
\vdots&\vdots&\ddots&\vdots\\
0&0&0&A_r
\end{pmatrix}
\]
with $A_i$ of order $|B_i| = \dim W_i$
\end{theorem}
\begin{theorem}[Cayley-Hamilton Theorem]
\hfill\\\normalfont
Let
\[
p_T(x)=|xI_n-[T]_B|=\sum_{i=0}^n b_ix^i
\]
be the characteristic polynomial of a linear operator
\[
T:V\to V\
\]
on an $n$-dimensional vector space $V$ over a field $\mathbb{F}$ and with a basis $B$. Then $T$ satisfies the equation $p_T(x)=0$, i.e.
\[
p_T(T)=0I_V
\]
which is the zero map on $V$.
\end{theorem}
\clearpage
\section{Minimal Polynomial and Jordan Canonical Form}
\begin{definition}[Minimal Polynomial]
\hfill\\\normalfont Let
\[
T:V\to V
\]
be a linear operator on an $n$-dimensional vector space over a field $\mathbb{F}$. A nonzero polynomial
\[
m(x)\in\mathbb{F}[x]
\]
is a \textbf{minimal polynomial of} $T$ if it satisfies:
\begin{enumerate}
\item $m(x)$ is monic,
\item Vanishing condition:
\[
m(T)=0I_V
\]
\item Minimality degree condition:\\Whenever $f(x)\in\mathbb{F}[x]$ is another nonzero polynomial such that $f(T)=0I_V$, we have
\[
\deg(f(x))\geq \deg(m(x))
\]
\end{enumerate}
We can define a \textbf{minimal polynomial of} a matrix $A\in\mathbb{M}_N(\mathbb{F})$.
\end{definition}
\textbf{Remark}: The existence of minimal polynomial is proven by Cayley-Hamilton theorem.
\begin{theorem}[Uniqueness of a minimal polynomial $m_T(x)$]
\hfill\\\normalfont Let $T:V\to V$ be a linear operator on an $n$-dimensional vector space $V$ over a field $\mathbb{F}$. Let $m(x)$ be a minimal polynomial of $T$. Let $f(x)\in\mathbb{F}[x]$. Then the following are equivalent.
\begin{enumerate}[label = (\arabic*)]
\item $f(T)=0I_V$.
\item $m(x)$ is a factor of $f(x)$,i.e., $m(x)\mid f(x)$.
\end{enumerate}
In particular, there is exactly one minimal polynomial of $T$ and will be denoted as
\[
m_T(x)=m(x)
\]
Further, if $A=[T]_B$, then $m_T(x)=m_A(x)$.
\end{theorem}
\begin{theorem}[Minimal polynomials of similar matrices]
\hfill\\\normalfont If two matrices $A_i$ are simialr: $A_1\sim A_2$, then they have the same minimal polynomial
\[
m_{A_1}(x)=m_{A_2}(x)
\]
\end{theorem}
\begin{theorem}[Minimal polynomials of direct sums]
\hfill\\\normalfont Consider the matrix 
\[
A=\begin{pmatrix}
A_1&0&\cdots&0\\
0&A_2&\cdots&0\\
\vdots&\vdots&\ddots&\vdots\\
0&0&\cdots&A_r
\end{pmatrix}
\]
where $A_i\in\mathbb{M}_{n_i}(\mathbb{F})$ are square matrices. The minimal polynomial $m_A(x)$ of $A$ is equal to the \textbf{least common multiple} of the minimal polynomials $m_{A_i}(x)$ of $A_i$,i.e.,
\[
m_A(x)=\lcm\{m_{A_1}(x),\ldots,m_{A_r}(x)\}
\]
\end{theorem}
\begin{theorem}
\hfill\\\normalfont The set of zeros of $p_T(x)$ and that of $m_T(x)$ are identical.
\end{theorem}
\begin{definition}[Jordan Block]
\hfill\\\normalfont Let $\lambda$ be a scalar in a field $\mathbb{F}$. The matrix below
\[
J:=J_s(\lambda)=\begin{pmatrix}
\lambda&1&0&\cdots&0\\
0&\lambda&1&\cdots&0\\
0&0&\lambda&\cdots&0\\
\vdots&\vdots&\vdots&\ddots&\vdots\\
0&0&0&\cdots&\lambda
\end{pmatrix}\in\mathbb{M}_s(\mathbb{F})
\]
is called the \textbf{Jordan Block} of order s with eigenvalue $\lambda$.\\
The characteristic polynomial and minimal polynomial of $J$ are identical:
\[
m_J(x)=(x-\lambda)^s = p_J(x)
\]
The eigenspace 
\[
V_\lambda(J)=\spn\{\mathbf{e}_1\}
\]
has dimension 1, i.e. the geometric multiplicity of $\lambda$  is 1, but the algebraic multiplicity of $\lambda$ is $s$.
\end{definition}
\begin{definition}[Jordan Canonical Form]
\hfill\\\normalfont Let $\lambda$ be a nonzero scalar in a field $\mathbb{F}$. Let
\[
s_1\leq s_2\leq \cdots\leq s_e
\]
The following \textbf{Block Diagonal}
\[
A(\lambda)=\begin{pmatrix}
J_{s_1}(\lambda)&0&0&\cdots&0\\
0&J_{s_2}(\lambda)&0&\cdots&0\\
0&0&J_{s_3}(\lambda)&\cdots&0\\
\vdots&\vdots&\vdots&\ddots&\vdots\\
0&0&0&\cdots&J_{s_e}(\lambda)
\end{pmatrix}
\]
is called a \textbf{Jordan canonical form with eigenvalue} $\lambda$.\\
The order of $A(\lambda)$ is 
\[
s=\sum_{i=1}^e s_i
\]
The characteristic polynomial and minimal polynomial of $A$ are
\[
p_{A(\lambda)}(x)=(x-\lambda)^s,\;\;\; m_{A(\lambda)}(x)=(x-\lambda)^{s_e}
\]
where $s$ is also called algebraic multiplicity of $\lambda$ of $A(\lambda)$.\\
The eigenspace of $A$
\[
V_\lambda(A(\lambda)) = \spn\{\mathbf{e}_1,\mathbf{e}_{1+s_1},\ldots,\mathbf{e}_{1+s_1+\cdots+s_{e-1}}\}
\]
has dimension equal to $e$.\\
The geometric multiplicity of the eigenvalue $\lambda$ of $A(\lambda)$ is $\dim V_\lambda(A(\lambda)) = e$. And we have,
\[
e\leq s
\]
More generally, let 
\[
\lambda_1,\ldots,\lambda_k
\]
be distinct scalars in $\mathbb{F}$. WLOG, we assume $\lambda_1<\lambda_2<\cdots<\lambda_k$ when $\mathbb{F}=\mathbb{R}$. Then the block diagonal
\[
J=\begin{pmatrix}
A(\lambda_1)&0&0&\cdots&0\\
0&A(\lambda_2)&0&\cdots&0\\
0&0&A(\lambda_3)&\cdots&0\\
\vdots&\vdots&\vdots&\ddots&\vdots\\
0&0&0&\cdots&A(\lambda_k)
\end{pmatrix}
\]
is called a \textbf{Jordan canonical form} where $A(\lambda_i)$ is a Jordan canonical form with eigenvalue $\lambda_i$ as shown above.\\
Each $A(\lambda_i)$ is of order 
\[
s(\lambda_i)
\]
and $s(\lambda_i)$ is also the number of times the same scalar $\lambda_i$ appears on the diagonal of $J$ and also the algebraic multiplicity of the eigenvalue $\lambda_i$ of $J$.\\
So $J$ is of order euqal to 
\[
\sum_{i=1}^k s(\lambda_i)
\]
There are exactly
\[
e(\lambda_i)
\]
Jordan blocks (with eigenvalue $\lambda_i$) in $A(\lambda_i)$, the largest of which is of order
\[
s_e(\lambda_i)
\]
This $s_e(\lambda_i)$ is also the multiplicity of $\lambda_i$ in the minimal polynomial $m_J(x)$.\\
There are exactly
\[
\sum_{i=1}^k e(\lambda_i)
\]
Jordan blocks in $J$.\\
Now we have
\[
\begin{aligned}
p_J(x)&=\prod_{i=1}^k(x-\lambda_i)^{s(\lambda_i)}\\
m_J(x)&=\prod_{i=1}^k(x-\lambda_i)^{s_e(\lambda_i)}
\end{aligned}
\]
The eigenspace $V_{\lambda_i}(J)$ has dimension $e(\lambda_i)$ and is spanned by the $e(\lambda_i)$ ectors corresponding to the first columns of the $e(\lambda_i)$ Jordan blocks in $A(\lambda_i)$.\\
We also have
\[
\dim V_{\lambda_i}(J)=e(\lambda_i)\leq s(\lambda_i)
\]
Sometimes, a block diagonal $J$ below
\[
J=\begin{pmatrix}
J_{s_1}(\lambda_1)&0&0&\cdots&0\\
0&J_{s_2}(\lambda_2)&0&\cdots&0\\
0&0&J_{s_3}(\lambda_3)&\cdots&0\\
\vdots&\vdots&\vdots&\ddots&\vdots\\
0&0&0&\cdots&J_{s_r}(\lambda_r)
\end{pmatrix}
\]
is also called a \textbf{Jordan Canonical Form}, where each $J_{s_i}(\lambda_i)$ is a Jordan block with eigenvalue $\lambda_i\in\mathbb{F}$, but these $\lambda_i$'s may not be distinct.\\
Assume that there are exactly $k$ distinct elements in the set 
\[
\{\lambda_1,\ldots,\lambda_r\}
\]
and we assume that 
\[
\lambda_{m_i}
\]
are these $k$ distinct ones. These $k$ of $\lambda_{m_i}$ are just the distinct eigenvalues of $J$.\\
Let
\[
s(\lambda_{m_i})
\]
be the number of times the same scalar $\lambda_{m_i}$ appears on the diagonal of $J$.
Let 
\[
e(\lambda_{m_i})
\]
be the number of Jordan blocks (among the $r$ such in $J$) with eigenvalue of the same $\lambda_{m_i}$; among these $e(\lambda_{m_i})$ Jordan blocks, the largest is of order say
\[
s_e(\lambda_{m_i})
\]
The eigenspace $V_{\lambda_{m_i}}(J)$ has dimension $e(\lambda_{m_i})$ and is spanned by $e(\lambda_{m_i})$ vectors corresponding to the first columns of these $e(\lambda_{m_i})$ Jordan blocks.\\
Also,
\[
\begin{aligned}
p_J(x)&=\prod_{i=1}^k(x-\lambda_i)^{s(\lambda_{m_i})}\\
m_J(x)&=\prod_{i=1}^k(x-\lambda_i)^{s_e(\lambda_{m_i})}
\end{aligned}
\]
As in the case of $A(\lambda)$, for the matrix $J$, we have
\[
\dim V_{\lambda_{m_i}}(J)=e(\lambda_{m_i})\leq s(\lambda_{m_i})
\]
\end{definition}
\begin{theorem}[Jordan Canonical Form of a Linear Operator]
\hfill\\\normalfont Let $V$ be a vector space of dimension $n$ over a field $\mathbb{F}$ and
\[
T:V\to V
\]
a linear operator with characteristic polynomial $p_T(x)$ and minimal polynomial $m_T(x)$ as follows
\[
\begin{aligned}
p_T(x)&=(x-\lambda_1)^{n_1}\cdots(x-\lambda_k)^{n_k}\\
m_T(x)&=(x-\lambda_1)^{m_1}\cdots (x-\lambda_k)^{m_k}
\end{aligned}
\]
where
\[
\lambda_1,\ldots,\lambda_k
\]
are distinct scalars in $\mathbb{F}$.\\
Then there is a basis $B$ of $V$ such that the representative matrix $[T]_B$ equals a Jordan canonical form $J\in\mathbb{M}_n(\mathbb{F})$, with
\[
n_i=s(\lambda_i),\;\;\;\;\;\;m_i=s_e(\lambda_i)
\]
Such a block diagonal $J$ is called a Jordan canonical form of $T$. It is unique up to re-ordering of $\lambda_i$. The basis $B$ of $V$ is called a Jordan canonical basis of $T$.
\end{theorem}
\begin{theorem}[A canonical form of $T$ is a canonical form of ${[T]_B}$]
\hfill\\\normalfont Let $V$ be an $n$-dimensional vector space over a field $\mathbb{F}$ and
\[
T:V\to V
\]
a linear operator. Let
\[
A={[T]}_B^\prime
\]
be the representation matrix of $T$ relative to a basis
\[
B^\prime=(\mathbf{v}_1,\ldots,\mathbf{v}_n)
\]
of $V$. Let $J$ be a Jordan canonical form. Then the following are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item There is an invertible matrix $P\in\mathbb{M}_n(\mathbb{F})$ such that
\[
P^{-1}AP=J
\]
\item THere is an invertible matrix $P\in\mathbb{M}_n(\mathbb{F})$ such that the representation matrix $[T]_B$ relative to the new basis
\[
B=B^\prime P
\]
is $J$, i.e.
\[
[T]_B=J
\]
\end{enumerate}
\end{theorem}
\begin{theorem}[Existence of Jordan Canonical Form]
\hfill\\\normalfont Let $\mathbb{F}$ be a field. Let $A$ be a matrix in $\mathbb{M}_n(\mathbb{F})$. Let $p(x) = p_A(x)$ be the characteristic polynomial. Then the following is equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $A$ has a Jordan canonical form $J\in \mathbb{M}_n(\mathbb{F})$.
\item Every zero of the characteristic polynomial $p(x)$ belongs to $\mathbb{F}$.
\item We can factor $p(x)$ as
\[
p(x)=(x-\lambda_1)\cdots(x-\lambda_n)
\]
where all $\lambda_i\in\mathbb{F}$.
\end{enumerate}
In particular, if $\mathbb{F}$ is so called \textbf{algebraically closed}, then every matrix $A\in\mathbb{M}_n(\mathbb{F})$ and every $T$ on an $n$-dimensional vector space $V$ over $\mathbb{F}$ have a Jordan canonical form $j\in\mathbb{M}_n(\mathbb{F})$.
\end{theorem}
\begin{theorem}[Consequences of Jordan canonical forms]
\hfill\\\normalfont Let $A$ be a matrix in $\mathbb{M}_n(\mathbb{F})$. Set $p(x)=p_A(x)$ and $m(x)=m_A(x)$.
\begin{enumerate}
\item The characteristic polynomial $p(x)$ and the minimal polynomial $m(x)$ have the same zero sets.
\[
\{\alpha\in\mathbb{F}\mid p(\alpha)=0\}=\{\alpha\in\mathbb{F}\mid m(\alpha)=0\}
\]
Also, the multiplicity $n_i$ and $m_i$ of a zero $\lambda_i$ of $p(x)$ and $m(x)$ satisfy
\[
n_i\geq m_i\geq 1
\]
\item If $J\in\mathbb{M}_n(\mathbb{F})$ is a Jordan canonical form of $A$, then we have
\[
\dim V_{\lambda_i}(A)=\dim V_{\lambda_i}(J)=e(\lambda_i)\leq s(\lambda_i)
\]
\end{enumerate}
\end{theorem}
\begin{theorem}[Canonical forms of similar matrices]
\hfill\\\normalfont Let $A_i\in\mathbb{M}_n(\mathbb{F})$ and $J_i\in\mathbb{M}_n(\mathbb{F})$ be its Jordan canonical form. Then the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $A_1$ and $A_2$ are similar.
\item We have $J_1=J_2$ after re-ordering of their Jordan Block.
\end{enumerate}
\end{theorem}
\begin{definition}[Diagonalisable Operator]
\hfill\\\normalfont Let $V$ be an $n$-dimensional vector space over a field $\mathbb{F}$. A linear operator $T:V\to V$ is \textbf{diagonalisable} over $\mathbb{F}$, if the representation matrix $[T]_B$ relative to some basis $B$ of $V$ is a diagonal matrix in $\mathbb{M}_n(\mathbb{F})$:
\[
[T]_b=J=\begin{pmatrix}
\lambda_1&0&0&\cdots&0\\
0&\lambda_2&0&\cdots&0\\
0&0&\lambda_3&\cdots&0\\
\vdots&\vdots&\vdots&\ddots&\vdots\\
0&0&0&\cdots&\lambda_n
\end{pmatrix}
\]
where $\lambda_i$ are scalars in $\mathbb{F}$. This $J$ is then an automatically a Jordan canonical form of $T$.\\
Clearly,
\[
\lambda_1,\ldots,\lambda_n
\]
exhaust all zeros of $p_T(x)$ and the characteristic polynomial of $T$ is
\[
p_T(x)=(x-\lambda_1)\cdots (x-\lambda_n)
\]
A square matrix $A\in\mathbb{M}_n(\mathbb{F})$ is diagonalisable over $\mathbb{F}$,  if $A$ is similar to a diagonal matrix in $\mathbb{M}_n(\mathbb{F})$, i.e.
\[
P^{-1}AP=J=\begin{pmatrix}
\lambda_1&0&0&\cdots&0\\
0&\lambda_2&0&\cdots&0\\
0&0&\lambda_3&\cdots&0\\
\vdots&\vdots&\vdots&\ddots&\vdots\\
0&0&0&\cdots&\lambda_n
\end{pmatrix}
\]
for some invertible $P\in\mathbb{M}_n(\mathbb{F})$, where $\lambda_i$ are scalars in $\mathbb{F}$. This $J$ is then automatically a Jordan canonical form of $A$.\\
Write 
\[
P=(\mathbf{p}_1,\ldots,\mathbf{p}_n)
\]
with $\mathbf{p}_j$ the $j$th column of $P$.\\
The diagonalisability condition on $A$ is equivalent to
\[
AP=P\begin{pmatrix}
\lambda_1&0&0&\cdots&0\\
0&\lambda_2&0&\cdots&0\\
0&0&\lambda_3&\cdots&0\\
\vdots&\vdots&\vdots&\ddots&\vdots\\
0&0&0&\cdots&\lambda_n
\end{pmatrix}
\]
i.e.
\[
(A\mathbf{p}_1,\ldots,A\mathbf{p}_n)=(\lambda_1\mathbf{p}_1,\ldots,\lambda_n\mathbf{p}_n)
\]
i.e. each $\mathbf{p}_i$ is an eigenvector of $A$ corresponding to the eigenvalue $\lambda_i$.\\
Suppose that $A=[T]_{B^\prime}$. Then the condition above is equivalent to 
\[
[T(\mathbf{v}_i)]_{B^\prime} = [T]_{B^\prime}[\mathbf{v}_i]_{B^\prime} = \lambda_i[\mathbf{v}_i]_{B^\prime}
\]
where $\mathbf{v}_i=B^\prime \mathbf{p}_i\in V$ with
\[
[\mathbf{v}_i]_{B^\prime} = \mathbf{p}_i
\]
i.e. 
\[
T(\mathbf{v}_i)=\lambda_i\mathbf{v}_i
\]
i.e. 
\[
T(\mathbf{v}_1,\ldots,\mathbf{v}_n)=(\mathbf{v}_1,\ldots,\mathbf{v}_n)\begin{pmatrix}
\lambda_1&0&0&\cdots&0\\
0&\lambda_2&0&\cdots&0\\
0&0&\lambda_3&\cdots&0\\
\vdots&\vdots&\vdots&\ddots&\vdots\\
0&0&0&\cdots&\lambda_n
\end{pmatrix}
\]
i.e. 
\[
[T]_B=\begin{pmatrix}
\lambda_1&0&0&\cdots&0\\
0&\lambda_2&0&\cdots&0\\
0&0&\lambda_3&\cdots&0\\
\vdots&\vdots&\vdots&\ddots&\vdots\\
0&0&0&\cdots&\lambda_n
\end{pmatrix}
\]
where
\[
B=(\mathbf{v}_1,\ldots,\mathbf{v}_n)=B^\prime P
\]
is a basis of $V$ since
\[
([\mathbf{v}_1]_{B^\prime},\ldots,[\mathbf{v}_n]_{B^\prime}) = (\mathbf{p}_1,\ldots,\mathbf{p}_n)
\]
is a basis of column vector space $\mathbb{F}_c^n$.
\end{definition}
\begin{theorem}
\hfill\\\normalfont
\begin{enumerate}[label=(\arabic*)]
\item $T$ is diagonalisable if and only if the representation matrix $[T]_{B^\prime}$ relative to every basis $B^\prime$ is diagonalisable.
\item A matrix $A\in\mathbb{M}_n(\mathbb{F})$ is diagonalisable if and only if the matrix transformation $T_A$ on the column $n$-space $\mathbb{F}_c^n$ is diagonalisable.
\end{enumerate}
\end{theorem}
\begin{theorem}[Equivalent Diagonalisable Condition]
\hfill\\\normalfont Let $V$ be an $n$-dimensional vector space over a field $\mathbb{F}$, and
\[
T:V\to V
\] 
a linear operator. Then the following are equivalent:
\begin{enumerate}
\item $T$ i s diagonalisable over $\mathbb{F}$, i.e. the representation matrix of $T$ relative to some basis $B$ of $V$ is a diagonal matrix in $\mathbb{M}_n(\mathbb{F})$.
\[
[T]_B=\begin{pmatrix}
\lambda_1&0&0&\cdots&0\\
0&\lambda_2&0&\cdots&0\\
0&0&\lambda_3&\cdots&0\\
\vdots&\vdots&\vdots&\ddots&\vdots\\
0&0&0&\cdots&\lambda_n
\end{pmatrix}
\]
\item $[T]_{B^\prime}$ is diagonalisable over $\mathbb{F}$ for every basis $B^\prime$ of $V$, i.e. there exists an invertible $P\in\mathbb{M}_n(\mathbb{F})$ such that
\[
P^{-1}[T]_{B^\prime}P=\text{diag}[\lambda_1,\ldots,\lambda_n]
\]
for some scalars $\lambda_i\in\mathbb{F}$(automatically being eigenvalues of $T$).
\item A basis
\[
B=(\mathbf{v}_1,\ldots,\mathbf{v}_n)
\]
of $V$ is formed by eigenvectors $\mathbf{v}_i$ of $T$.
\item There are $n$ linearly independent eigenvectors $\mathbf{v}_i$ of $T$.
\item For the representation matrix $[T]_{B^\prime}$ relative to every basis $B^\prime$ of $V$, a basis
\[
P=(\mathbf{p}_1,\ldots,\mathbf{p}_n)
\]
of the column $n$-space $\mathbb{F}_c^n$ is formed by eigenvectors $\mathbf{p}_i$ of $[T]_{B^\prime}$.
\item For the representation matrix $[T]_{B^\prime}$ relative to every basis $B^\prime$ of $V$, there are $n$ linearly independent eigenvectors $\mathbf{p}_i$ of $[T]_{B^\prime}$.
\item Let
\[
\lambda_{m_1},\ldots,\lambda_{m_k}
\]
be the only distinct eigenvalues of $T$ and let $B_i$ be a basis of the eigenspace $V_{\lambda_i}(T)$. Then
\[
B=(B_1,\ldots, B_k)
\]
is a basis of $V$, automatically with 
\[
[T]_B =\begin{pmatrix}
\lambda_{m_1}I_{|B_1|}&0&0&\cdots&0\\
0&\lambda_{m_2}I_{|B_2|}&0&\cdots&0\\
0&0&\lambda_{m_3}I_{|B_3|}&\cdots&0\\
\vdots&\vdots&\vdots&\ddots&\vdots\\
0&0&0&\cdots&\lambda_{m_k}I_{|B_k|}
\end{pmatrix}
\]
\item Let \[
\lambda_{m_1},\ldots,\lambda_{m_k}
\]
be the only distinct eigenvalues of $T$. Then $V$ is a direct sum of the eigenspaces
\[
V=V_{\lambda_{m_i}}(T)\oplus\cdots\oplus V_{\lambda_{m_k}}(T)
\]
\item  Let \[
\lambda_{m_1},\ldots,\lambda_{m_k}
\]
be the only distinct eigenvalues of $T$. Then
\[
\sum_{i=1}^k\dim V_{\lambda_{m_i}}(T)=\dim V
\]
\item $T$ has a Jordan canonical form $J$ which is diagonal.
\end{enumerate}
\end{theorem}
\begin{theorem}[Minimal polynomial and diagonalisability]
\hfill\\\normalfont Let $\mathbb{F}$ be a field. Let $A$ be a matrix in $\mathbb{M}_n(\mathbb{F})$. Let $m(x)=m_A(x)$ be minimal polynomial of $A$. Then the following are equivalent:
\begin{enumerate}[label=(\arabic*)]
\item $A$ is diagonalisable over $\mathbb{F}$.
\item The minimal polynomial $m(x)$ is a product of distinct linear polynomials in $\mathbb{F}[x]$.
\[
m(x)=(x-\lambda_1)\cdots(x-\lambda_k)
\]
where $\lambda_i$ are distinct scalars in $\mathbb{F}$
\item We can factor $m(x)$ over $\mathbb{F}$ as
\[
m(x)=(x-\lambda_1)\cdots(x-\lambda_k)
\]
for some scalars $\lambda_i\in\mathbb{F}$ and $m(x)$ has only simple zeros.
\item Let $p(x)=p_A(x)$ be the characteristic polynomial. Then we can factorise $p(x)$ over $\mathbb{F}$ as
\[
p(x)=(x-\lambda_1)^{n_1}\cdots(x-\lambda_k)^{n_k}
\]
where $\lambda_i$ are distinct scalars in $\mathbb{F}$. The dimension of the eigenspace satisfies:
\[
\dim V_{\lambda_i}=n_i
\]
\end{enumerate}
\end{theorem}
\begin{theorem}\hfill\\\normalfont Let $V$ be an $n$-dimensional vector space over a field $\mathbb{F}$.\\A linear operator 
\[
T:V\to V
\]
on $V$ is \textbf{nilpotent} if
\[
T^m=0I_V
\]
for some positive integer $m$.\\
Suppose that $T$ has a Jordan canonical form $J\in\mathbb{M}_n(\mathbb{F})$. The following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $T$ is nilpotent.
\item $J$ equals some $A(\lambda)$ with $\lambda=0$.
\item Every eigenvalue of $T$ is zero/
\item The characteristic polynomial of $T$ is $p_T(x)=x^n$.
\item The minimal polynomial of $T$ is $m_T(x)=x^s$ for some $s\geq 1$.
\end{enumerate}
\end{theorem}
\begin{theorem}[Additive Jordan Decomposition]
\hfill\\\normalfont Suppose a linera operator 
\[
T:V\to V
\]
has a Jordan canonical form in $\mathbb{M}_n(\mathbb{F})$. There are linear operators
\[
T_S:V\to V
\]
and
\[
T_n:V\to V
\]
satisfying the following:
\begin{enumerate}[label=(\arabic*)]
\item A decomposition
\[
T=T_s+T_n
\]
\item $T_s$ is semi-simple.
\item $T_n$ is nilpotent.
\item Commutativity
\[
T_s\circ T_n  = T_n\circ T_s
\]
\item  There are polynomials $f(x),g(x)$ in $\mathbb{F}[x]$ such that
\[
T_s=f(T)\;\;\;\;\;T_n=g(T)
\]
\end{enumerate}
This decomposition is unique. We call it Jordan decomposition.
\end{theorem}
\clearpage
\section{Quadratic Forms, Inner Product Spaces and Conics}
\begin{definition}[Bilinear forms]
\hfill\\\normalfont Let $V$ be a vector space over a field $\mathbb{F}$. Consider the map $H$ below:
\[
\begin{aligned}
H:V\times V&\to \mathbb{F}\\
(\mathbf{x},\mathbf{y})\mapsto H(\mathbf{x},\mathbf{y})
\end{aligned}
\]
\begin{enumerate}[label=(\arabic*)]
\item $H$ is called a \textbf{bilinear form on} $V$ if $H$ is linear in both variables, i.e., for all
\[
\mathbf{x}_i, \mathbf{y}_j,\mathbf{x},\mathbf{y}\in V,\;a_i,b_i\in\mathbb{F}
\]
we have
\[
\begin{aligned}
H(a_1\mathbf{x}_1+a_2\mathbf{x}_2,\mathbf{y})&=a_1H(\mathbf{x}_1,y)+a_2H(\mathbf{x}_2,y)\\
H(\mathbf{x},b_1\mathbf{y}_1+b_2\mathbf{y}_2)&=b_1H(\mathbf{x},\mathbf{y}_1)+b_2H(\mathbf{x},\mathbf{y}_2)
\end{aligned}
\]
\item A bilinear form $H$ on $V$ is \textbf{symmetric} if
\[
H(\mathbf{x},\mathbf{y})=H(\mathbf{y},\mathbf{x})\;\;\;\forall \mathbf{x},\mathbf{y}\in V
\]
\end{enumerate}
\end{definition}
\begin{theorem}[Representation Matrix]
\hfill\\\normalfont Suppose that
\[
B=(\mathbf{v}_1,\ldots,\mathbf{v}_n)
\]
is a basis of a vector space $V$ over a field $\mathbb{F}$. Let
\[
A=(a_{ij})=\begin{pmatrix}
a_{11}&\cdots&a_{1n}\\
\vdots&\ddots&\vdots\\
a_{n1}&\cdots&a_{nn}
\end{pmatrix}
\]
be a matrix in $mathbb{M}_n(\mathbb{F})$.\\
We define the function
\[
\begin{aligned}
H_A:V\times V&\to\mathbb{F}\\
(\sum_{i=1}^n x_i\mathbf{v}_i,\sum_{j=1}^n y_i\mathbf{v}_j)\mapsto H_A(\sum_{i=1}^n x_i\mathbf{v}_i,\sum_{j=1}^n y_i\mathbf{v}_j)
\end{aligned}
\]
where
\[
\begin{aligned}
& H_A(\sum_{i=1}^n x_i\mathbf{v}_i,\sum_{j=1}^n y_i\mathbf{v}_j)\\
:=&\sum_{i=1}^n\sum_{j=1}^n a_{ij}x_i y_j\\
=&(x_1,\ldots,x_n)\begin{pmatrix}
a_{11}&\cdots&a_{1n}\\
\vdots&\ddots&\vdots\\
a_{n1}&\cdots&a_{nn}
\end{pmatrix}\begin{pmatrix}
y_1\\
\vdots\\
y_n
\end{pmatrix}\\
=&X^tAY
\end{aligned}
\]
\begin{enumerate}[label=(\arabic*)]
\item Then $H_A$ is a bilinear form on $V$ and called the \textbf{bilinear form associated with }$A$ (and relative to the basis $B$ of $V$).
\item Conversely, every bilinear form $H$ on $V$ is of the form $H_A$ for some $A$ in $\mathbb{M}_n(\mathbb{F})$. Indeed, just set 
\[
a_{ij}=H(\mathbf{v}_i,\mathbf{v}_j),\;\;\; A:=(a_{ij})
\]
Then one can use the bilinearity of $H$, show that $H=H_A$.\\
The matrix $A$ is called the representation matrix of $H$ relative to the basis of $B$>
\item $H_A$ is a symmetric bilinear form if and only if $A$ is a \textbf{symmetric matrix}.
\end{enumerate}
\end{theorem}
\begin{definition}[Non-degenerate bilinear forms]
\hfill\\\normalfont A bilinear form $H$ on $V$ is \textbf{non-degenerate} if for every $\mathbf{y}_0\in V$, we have:
\[
H(\mathbf{x},\mathbf{y}_0)=0(\forall \mathbf{x}\in V)\Rightarrow \mathbf{y}_0=\mathbf{0}
\]
A bilinear form $H=H_A$ is non-degenerate if and only if its representation matrix $A$ is invertible.
\end{definition}
\begin{definition}[Congruent matrices]
\hfill\\\normalfont Two matrices $A$ and $B$ in $\mathbb{M}_n(\mathbb{F})$ are \textbf{congruent} if there is an invertible matrix $P\in\mathbb{M}_n(\mathbb{F})$ such that
\[
B=P^tAP
\]
Being congruent is an equivalent relation.\\
Consider the bilinear form
\[
\begin{aligned}
H:\mathbb{F}_c^n\times \mathbb{F}_c^n&\to \mathbb{F}\\
(X,Y)\mapsto X^tAY
\end{aligned}
\]
If we write
\[
X=PY
\]
with an invertible matrix $P\in\mathbb{M}_n(\mathbb{F})$ and introduce $Y$ as a new coordinate system for $\mathbb{F}_c^n$, then
\[
\begin{aligned}
H(X_1,X_2)&=X_1^tAX_2\\
&=(PY_1)^tA(PY_2)\\
&=Y_1^t(P^tAP)Y_2
\end{aligned}
\]
Thus, the bilinear form above would have a simpler form in new coordinates $Y$, if $P^tAP$ (which is congruent to $A$) is simpler. This simplification is very useful in classfying all conics.
\end{definition}
\begin{theorem}[Weak version of Principle Axis Theorem]
\hfill\\\normalfont Let $A\in\mathbb{M}_n(\mathbb{F})$ be a symmetric matrix. Then there is an invertible matrix $P$ in $\mathbb{M}_n(\mathbb{F})$ such that the matrix $P^tAP$ is diagonal:
\[
P^tAP=\text{diag}[d_1,\ldots, d_n]=:D
\]
i.e. $A$ is congruent to a diagonal matrix $D$.\\
In this case, the bilinear form
\[
\begin{aligned}
H(X_1, X_2)&=\sum_{i=1}^n\sum_{j=1}^n a_{ij}x_iy_j\\
&=X_1^tAX_2\\
&=Y_1^tDY_2\\
&=(y_{11},\ldots,y_{1n})\begin{pmatrix}
d_{1}&\cdots&0\\
0&\ddots&0\\
0&\cdots&d_{n}
\end{pmatrix}\begin{pmatrix}
y_21\\
\vdots\\
y_2n
\end{pmatrix}\\
&=\sum_{j=1}^n d_jy_{1j}y_{2j}
\end{aligned}
\]
where we have used the substitution:
\[
X_i=PY_i
\]
\end{theorem}
\begin{definition}[Inner Product, Orthogonal, Norm]
\hfill\\\normalfont
We start with the real version.\\
Consider a function $H$:
\[
\begin{aligned}
V\times V&\to \mathbb{R}\\
(\mathbf{x},\mathbf{y})&\mapsto H(\mathbf{x},\mathbf{y})
\end{aligned}
\]
on a vector space $V$ over the field $\mathbb{R}$ of real numbers.\\
The function $H$ is called a \textbf{real inner product} and $V$ a \textbf{real inner product space}, if the following three conditions are satisfied, where we denote
\[
\langle\mathbf{x},\mathbf{y}\rangle:=H(\mathbf{x},\mathbf{y})
\]
\begin{enumerate}[label=(\arabic*)]
\item $H$ is a bilinear form, i.e., for all
\[
\mathbf{x}_i, \mathbf{y}_j, \mathbf{x}, \mathbf{y}\in V, a_i, b_i\in\mathbb{R}
\]
we have
\[
\begin{aligned}
\langle a_1\mathbf{x}_1+a_2\mathbf{x}_2,\mathbf{y}\rangle &= a_1\langle \mathbf{x}_1, \mathbf{y}\rangle+a_2\langle \mathbf{x}_2, \mathbf{y}\rangle\\
\langle \mathbf{x}, b_1\mathbf{y}_1+b_2\mathbf{y}_2\rangle &= b_1\langle \mathbf{x}, \mathbf{y}_1\rangle+b_2\langle \mathbf{x}, \mathbf{y}_2\rangle
\end{aligned}
\]
\item $H$ is symmetric, i.e., for all $\mathbf{x},\mathbf{y}\in V$, we have
\[
\langle \mathbf{x},\mathbf{y}\rangle = \langle \mathbf{y},\mathbf{x}\rangle
\]
\item Positivity:\\
For all $\mathbf{0}\neq \mathbf{x}\in V$, we have
\[
\langle \mathbf{x},\mathbf{x}\rangle >0
\]
\end{enumerate}
Next is the complex version.
Consider a function $H$:
\[
\begin{aligned}
V\times V&\to \mathbb{C}\\
(\mathbf{x},\mathbf{y})&\mapsto H(\mathbf{x},\mathbf{y})
\end{aligned}
\]
on a vector space $V$ over the field $\mathbb{C}$ of real numbers.\\
The function $H$ is called a \textbf{complex inner product} and $V$ a \textbf{complex inner product space}, if the following three conditions are satisfied, where we denote
\[
\langle\mathbf{x},\mathbf{y}\rangle:=H(\mathbf{x},\mathbf{y})
\]
\begin{enumerate}[label=(\arabic*)]
\item $H$ is a bilinear form, i.e., for all
\[
\mathbf{x}_i, \mathbf{y}_j, \mathbf{x}, \mathbf{y}\in V, a_i, b_i\in\mathbb{R}
\]
we have
\[
\begin{aligned}
\langle <a_1\mathbf{x}_1+a_2\mathbf{x}_2,\mathbf{y}\rangle &= a_1\langle \mathbf{x}_1, \mathbf{y}\rangle+a_2\langle \mathbf{x}_2, \mathbf{y}\rangle\\
\langle <\mathbf{x}, b_1\mathbf{y}_1+b_2\mathbf{y}_2\rangle &= \bar{b}_1\langle \mathbf{x}, \mathbf{y}_1\rangle+\bar{b}_2\langle \mathbf{x}, \mathbf{y}_2\rangle
\end{aligned}
\]
\item $H$ is symmetric, i.e., for all $\mathbf{x},\mathbf{y}\in V$, we have
\[
\langle \mathbf{x},\mathbf{y}\rangle = \overline{\langle \mathbf{y},\mathbf{x}\rangle}
\]
\item Positivity:\\
For all $\mathbf{0}\neq \mathbf{x}\in V$, we have
\[
\langle \mathbf{x},\mathbf{x}\rangle >0
\]
\end{enumerate}
We have three more definitions:
\begin{enumerate}[label=(\arabic*)]
\item The \textbf{norm} of a vector $\mathbf{x}\in V$ is denoted and defined as:
\[
\norm{\mathbf{x}}=\sqrt{\langle \mathbf{x},\mathbf{x}\rangle}
\]
We have
\[
\norm{\mathbf{x}}\geq 0
\]
and
\[
\norm{\mathbf{x}}==0\Leftrightarrow \mathbf{x}=\mathbf{0}_V
\]
\item Two vectors $\mathbf{x}, \mathbf{y}$ in $V$ are \textbf{orthogonal} to each other and denoted as
\[
\mathbf{x}\perp \mathbf{y}
\]
if their inner product
\[
\langle \mathbf{x}, \mathbf{y}\rangle = 0
\]
\item Sometimes, we use
\[
(V,\langle,\rangle)
\]
to denote a vector space $V$ with an inner product $\langle,\rangle$.
\end{enumerate}
\end{definition}
\begin{definition}[Non-degenerate Inner Product]
\hfill\\\normalfont Let $(V,\langle,\rangle)$ be an inner product space over a field $\mathbb{F}$ with $\mathbb{F}=\mathbb{R}$ or $\mathbb{F}=\mathbb{C}$. Then the product $\langle,\rangle$ is \textbf{non-degenerate} in the sense:\\
for every $\mathbf{u}_0\in V$
\[
\langle\mathbf{u}_0,\mathbf{y}\rangle = 0 (\forall \mathbf{y}\in V)\Rightarrow \mathbf{u}_0=\mathbf{0}_V
\]
and for every $\mathbf{v}_0\in V$,
\[
\langle\mathbf{x},\mathbf{v}_0\rangle = 0 (\forall \mathbf{x}\in V)\Rightarrow \mathbf{v}_0=\mathbf{0}_V
\]
\end{definition}
\begin{definition}[Orthonormal basis]
\hfill\\\normalfont Let $(V,\langle,\rangle)$ be a real or complex inner product space. A basis $B = (\mathbf{v}_1,\ldots,\mathbf{v}_n$ is called an orthonormal basis of the inner product space $V$, if it satisfies the following two conditions:
\begin{enumerate}[label =(\arabic*)]
\item \textbf{Orthogonality}:\\
for all $i\neq j$, we have:
\[
\mathbf{v}_i\perp \mathbf{v}_j\;\;\;\text{i.e., }\;\;\;\langle\mathbf{v}_i,\mathbf{v}_j\rangle = 0
\] 
\item \textbf{Normalised}:\\
for all $i$, we have
\[
\norm{\mathbf{v}_i}=1
\]
Namely, $\mathbf{v}_i$ is a unit vector.
\end{enumerate}
\end{definition}
\begin{theorem}[Gram-Schmidt Process]
\hfill\\\normalfont Let $V=\mathbb{F}_c^n$ with $\mathbb{F}=\mathbb{R}$ or $\mathbb{C}$. Employ the standard inner product $\langle,\rangle$ for $V$.\\
Let 
\[
(\mathbf{u}_1,\ldots,\mathbf{u}_r)
\]
be a basis of a subspace $W$ of $V$. Then one can apply the following \textbf{Gram-Schmidt process} to get an orthonormal basis
\[
(\mathbf{v}_1,\ldots,\mathbf{v}_r)
\]
of $W$.
\[
\begin{aligned}
\mathbf{v}_1^\prime &= \mathbf{u}_1\\
\mathbf{v}_2^\prime&=\mathbf{u}_2 -\frac{\langle \mathbf{u}_2, \mathbf{v}_1^\prime\rangle}{\norm{\mathbf{v}_1^\prime}^2}\mathbf{v}_1^\prime\\
\mathbf{v}_k^\prime = \mathbf{u}_k-\sum_{i=1}^{k-1}\frac{\langle\mathbf{u}_k, \mathbf{v}_i^\prime\rangle}{\norm{\mathbf{v}_i^\prime}^2}\\
\mathbf{v}_j=\frac{\mathbf{v}_j^\prime}{\norm{\mathbf{v}_j^\prime}}
\end{aligned}
\]
\end{theorem}
\begin{definition}[Adjoint matrices $A^\ast$]
\hfill\\\normalfont For a matrix $A=(a_{ij})\in\mathbb{M}_n(\mathbb{C})$, the \textbf{adjoint} of $A$ is defined as
\[
A^\ast = (\overline{A})^t=(\overline{a_{ij}})^t
\]
i.e., the $(i,j)$-entry of $A^\ast$ equals $\overline{a_{ji}}$. Note that
\[
A^\ast=\overline{(A^t)}
\]
\end{definition}
\begin{theorem}[Adjoint matrix $A^\ast$ and inner product]
\hfill\\\normalfont Let $V=\mathbb{F})_c^n$ with $\mathbb{F}=\mathbb{R}$ or $\mathbb{C}$. Employ the standard inner product $\langle,\rangle$ for $V$. For a matrix $A\in\mathbb{M}_n(\mathbb{F})$, we have
\[
\langle AX,Y\rangle = \langle X,A^\ast Y\rangle
\]
\end{theorem}
\begin{theorem}[Adjoint Linear Operator]
\hfill\\\normalfont Let $T:V\to V$ be a linear operator on an $n$-dimensional inner product space $V$ over a field $\mathbb{F}$. Then we have:
\begin{enumerate}[label=(\arabic*)]
\item There is a \textbf{unique} linear operator
\[
T^\ast:V\to V
\]
on $V$ such that
\[
\langle T(\mathbf{u}), \mathbf{v}\rangle = \langle\mathbf{u},T^\ast(\mathbf{v})\rangle
\]
Such $T^\ast$ is called the \textbf{adjoint linear operator} of $T$.
\item Let $B=(\mathbf{w}_1, \ldots, \mathbf{w}_n)$ be an orthonormal basis of the inner product space $V$. Then
\[
[T^\ast]_B=([T]_B)^\ast
\]
\end{enumerate}
\end{theorem}
\begin{theorem}[Adjoint of adjoint]$(T^\ast)^\ast = T$.
\end{theorem}
\begin{theorem}[Adjoint of linear map combinations]\hfill\\\normalfont
\begin{enumerate}[label=(\arabic*)]
\item Suppose that $T=\alpha I_V$ is a scalar map. Then
\[
T^\ast = \overline{\alpha}I_V 
\]
\item \[
(a_1T_1+a_2T_2)^\ast = \overline{a_1}T_1^\ast +\overline{a_2}T_2^\ast
\]
\item
\[
(T_1\circ T_2)^\ast = T_2^\ast \circ T_1^\ast
\]
\end{enumerate}
\end{theorem}
\begin{definition}[Orthogonal, Unitary, Self-adjoint, Normal linear operators]
\hfill\\\normalfont Let $A\in\mathbb{M}_n(\mathbb{C})$ (resp. let $T:V\to V$ be a linear operator on an $n$-dimensional inner product space over a field $\mathbb{F}=\mathbb{R}$ or $\mathbb{C}$ and with an orthonormal basis $B$). Let $A^\ast$ (resp. $T^\ast$) be the adjoint of $A$ (resp. $T$). 
\begin{enumerate}[label=(\arabic*)]
\item A linear operator $T$ over a real inner product space is \textbf{orthogonal} if
\[
TT^\ast = I_V
\]
\item A real matrix $A$ in $\mathbb{M}_n(\mathbb{R})$ is \textbf{orthogonal} if
\[
AA^t = I_n
\]
\item A linear operator $T$ over a complex inner product space is \textbf{unitary} if
\[
TT^\ast = I_V
\]
\item A complex matrix $A$ in $mathbb{M}_n(\mathbb{C})$ is \textbf{unitary} if
\[
AA^\ast = I_n
\]
\item $T$ is \textbf{self-adjoint} if its adjoint $T^\ast$ equals itself:
\[
T=T^\ast
\]
When the field $\mathbb{F}=\mathbb{R}$, a self adjoint operator is also called a \textbf{symmetric} operator.
\item A complex matrix $A\in\mathbb{M}_n(\mathbb{C})$ is \textbf{self-adjoint} if the adjoint matrix of $A$ equals itself:
\[
A^\ast = A
\]
\item A linear operator $T$ over a complex inner product space is \textbf{normal} if
\[
TT^\ast = T^\ast T
\]
\item A complex matrix $A\in\mathbb{M}_n(\mathbb{C})$ is \textbf{normal} if
\[
AA^\ast = A^\ast A
\]
\end{enumerate}
Orthogonal, Unitary, self-adjoint operators are normal.
\end{definition}
\begin{theorem}\normalfont $T$ is orthogonal, unitary, self-adjoint or normal if and only if its representation matrix $A:=[T]_B$ (relative to one hence every orthonormal basis $B$) is respectively orthogonal, unitary, self-adjoint and normal.
\end{theorem}
\begin{theorem}[Equivalent unitary matrix definition]
\hfill\\\normalfont For a real matrix $P$ in $\mathbb{M}_n(\mathbb{C})$, the following are equivalent, if we employ the standard inner product on $\mathbb{C}_c^n$.
\begin{enumerate}[label=(\arabic*)]
\item $P$ is unitary, i.e. $PP^\ast = I_n$.
\item Write 
\[
P=(\mathbf{p}_1,\ldots, \mathbf{p}_n)
\]
where the $\mathbf{p}_j$ are the column vectors of $P$. Then the column vectors $\mathbf{p}_1,\ldots, \mathbf{p}_n$ form an orthonormal basis of $\mathbb{C}_c^n$.
\item The matrix transformation
\[
\begin{aligned}
T_P:\mathbb{C}_c^n&\to \mathbb{C}_c^n\\
X&\mapsto PX
\end{aligned}
\]
preserves the standard inner product, i.e., for all $X,Y$ in $\mathbb{C}_c^n$, we have
\[
\langle PX, PY\rangle = \langle X,Y\rangle
\]
\item The matrix transformation $T_P$ preserves the distance, i.e. for all $X,Y$ in $\mathbb{C}_c^n$, we have
\[
\norm{PX-PY}=\norm{X-Y}
\]
\item The matrix transformation $T_P$ preserves the norm, i.e. for all $X$ in $\mathbb{C}_c^n$, we have
\[
\norm{PX}=\norm{X}
\]
\item For one and hence every orthogonal basis
\[
B=(\mathbf{v}_1,\ldots, \mathbf{v}_n)
\]
of $\mathbb{C}_c^n$, the new basis
\[
B^\prime = BP
\]
is again an orthonormal basis of $\mathbb{C}_c^n$.
\end{enumerate}
\end{theorem}
\begin{theorem}[Eigenvalues of orthogonal or unitary matrices]
\hfill\\\normalfont
\begin{enumerate}[label = (\arabic*)]
\item If a real matrix $P\in\mathbb{M}_n(\mathbb{R})$ is orthogonal, then every zero of $p_P(x)$ has modulus equal to 1. In particular, the determinant
\[
|P|=\pm 1
\]
\item If a complex matrix $P\in\mathbb{M}_n(\mathbb{C})$ is unitary, then every eigenvalue
\[
\lambda_i =r_1+r_2\sqrt{-1}
\]
of $P$ has \textbf{modulus}
\[
|\lambda| = \sqrt{r_1^2+r_2^2}=1
\]
In particular, the determinant $|P|\in\mathbb{C}$ has modulus 1.
\end{enumerate}
\end{theorem}
\begin{theorem}[Eigenvalue of self-adjoint linear operators]
\hfill\\\normalfont
\begin{enumerate}[label=(\arabic*)]
\item Suppose that a real matrix $A\in\mathbb{M}_n(\mathbb{R})$ is symmetric. Every zero of $p_A(x)$ is a real number.
\item Suppose a complex matrix $A\in\mathbb{M}_n(\mathbb{C})$ is self-adjoint. Every zero of $p_A(x)$ is a real number.
\item More generally, suppose that $T$ is a self adjoint linear operator. Every zero of $p_T(x)$ is a real number.
\item Suppose that $T$ is a self-adjoint linear operator. Let $\mathbf{v}_i(i=1,2)$ be two eigenvectors corresponding to two distinct eigenvalues $\lambda_i$ of $T$. We have
\[
\langle \mathbf{v}_1,\mathbf{v}_2\rangle = 0
\]
\end{enumerate}
\end{theorem}
\begin{definition}[Positive/Negative definite linear operators]
\hfill\\\normalfont Let $A\in\mathbb{M}_n(\mathbb{C})$ (resp. let $V$ be an $n$-dimensional inner product space which is over a field $\mathbb{F}=\mathbb{R}$ or $\mathbb{C}$).
\begin{enumerate}[label=(\arabic*)]
\item $T$ is \textbf{positive definite} if $T$ is self-adjoint and
\[
\langle T(\mathbf{v}),\mathbf{v}\rangle >0
\]
\item $T$ is \textbf{negative definite} if $T$ is self-adjoint and
\[
\langle T(\mathbf{v}),\mathbf{v}\rangle <0
\]
Thus $T$ is negative definite if and only if $-T$ is positive definite.
\item $A$ is \textbf{positive definite} if $A$ is self-adjoint and
\[
(AX)^t\overline{X}=X^tA^t\overline{X}>0
\]
\item $A$ is \textbf{negative definite} if $A$ is self-adjoint and
\[
(AX)^t\overline{X}=X^tA^t\overline{X}<0
\]
Thus, $A$ is negative definite if and only if $-A$ is positive definite.
\end{enumerate}
\end{definition}
\begin{theorem}[Equivalent Positive-Definite Definition]
\hfill\\\normalfont Let
\[
A=(a_{ij})\in\mathbb{M}_n(\mathbb{R})
\]
be a symmetric real matrix. Then $A$ is positive definite if and only if all its \textbf{principal minors}
\[
(a_{ij})_{1\leq i,j\leq r}(1\leq r\leq n)
\]
of order $r$ have positive determinants.\\
Let $T$ be a self-adjoint linera operator on an inner product space $V$ which is over $\mathbb{F}=\mathbb{R}$ or $\mathbb{C}$ and with an orthonormal basis $B$. Set
\[
A:=[T]_B\in\mathbb{M}_n(\mathbb{F})
\]
Then the following are equivalent.
\begin{enumerate}[label=(\arabic*)]
\item $T$ is positive definite.
\item $A$ is positive definite.
\item Every eigenvalue of $T$ is positive.
\item Every eigenvalue of $A$ is positive.
\item One can write $A$ as
\[
A=C^\ast C
\]
for some invertible complex matrix $C\in\mathbb{M}_n(\mathbb{C})$
\end{enumerate}
\end{theorem}
\begin{theorem}\normalfont Let $A\in\mathbb{M}_n(\mathbb{C})$. The function $H$ on $V:=\mathbb{C}_c^n$
\[
\begin{aligned}
H:V\times V&\to \mathbb{C}\\
(X,Y)&\mapsto \langle X,Y\rangle :=(AX)^t \overline{Y}
\end{aligned}
\]
defines an inner product on $V$ if and only if $A$ is positive definite.
\end{theorem}
\begin{theorem}[Principle Axis Theorem]
\hfill\\\normalfont
\begin{enumerate}
\item Let $T:V\to V$ be a linear operator on a real inner product space $V$ of dimension $n$. Then $T$ is self-adjoint (i.e., $T^\ast = T$) if and only if there is an orthonormal basis $B$ such that 
\[
[T]_B
\]
is a diagonal matrix in $\mathbb{M}_n(\mathbb{R})$.
\item A real matrix $A\in\mathbb{M}_n(\mathbb{R})$ is self-adjoint (i.e. $A^\ast = A$) if and only if there is an orthogonal matrix $P$ such that
\[
P^{-1}AP=P^t AP
\]
is a diagonal matrix in $\mathbb{M}_n(\mathbb{R})$.
\item Let $T:V\to V$ be a linear operator on a complex inner product space $V$ of dimension $n$. Then $T$ is self-adjoint (i.e., $T^\ast = T$) if and only if there is an orthonormal basis $B$ such that 
\[
[T]_B
\]
is a diagonal matrix in $\mathbb{M}_n(\mathbb{C})$.
\item A complex matrix $A\in\mathbb{M}_n(\mathbb{C})$ is self-adjoint (i.e. $A^\ast = A$) if and only if there is an unitary matrix $U$ such that
\[
U^{-1}AU=U^\ast AU
\]
is a diagonal matrix in $\mathbb{M}_n(\mathbb{C})$.
\end{enumerate}
\end{theorem}
\begin{theorem}[Orthogonal Complement]
\hfill\\\normalfont Let $W$ be a subspace of an inner product space $V$. Take an orthogonal basis $B_W$ of $W$.
\begin{enumerate}[label=(\arabic*)]
\item One can extend $B_W$ to an orthonormal basis $B = (B_W, B_2)$ of $V$.
\item $B_2$ is an orthonormal basis of so called \textbf{orthogonal complement} of $W$:
\[
W^\perp :=\{\mathbf{x}\in V\mid \langle \mathbf{x},\mathbf{v}\rangle =0,\forall \mathbf{w}\in W\}
\]
\item 
\[
V=W\oplus W^\perp
\]
\end{enumerate}
\end{theorem}
\begin{definition}[Quadratic Form]
\hfill\\\normalfont Let $V$ be a vector space over a field $\mathbb{F}$. A function
\[
K:V\to \mathbb{F}
\]
or simply $K(\mathbf{x})$ is a \textbf{quadratic form} if there is a symmetric bilinear form
\[
H:V\times V\to \mathbb{F}
\]
such that
\[
K(\mathbf{x})=H(\mathbf{x},\mathbf{x})
\]
\end{definition}
\begin{theorem}[Principle Axis Theorem of Quadratic Form]
\hfill\\\normalfont Let 
\[
f(x_1,\ldots, x_n) = \sum_{i=1}^n\sum_{j=1}^n a_{ij}x_ix_j
\]
be a quadratic form in coordinates
\[
X=\begin{pmatrix}
x_1\\
\vdots\\
x_n
\end{pmatrix}
\]
with 
\[
A=(a_{ij})\in\mathbb{M}_n(\mathbb{R})
\]
a symmetric matrix. Then there is an orthogonal matrix $P$ such that $f$ has the following \textbf{standard form}
\[
f(x_1,\ldots, x_n)=\lambda_1 y_1^2+\cdots+\lambda_n y_n^2
\]
in the new coordinates
\[
Y:=\begin{pmatrix}
y_1\\
\vdots\\
y_n
\end{pmatrix}=P^{-1}X
\]
where $\lambda_i\in\mathbb{R}$ are the eigenvalues of $A$.\\
This standard form is unique up to relabelling of $\lambda_i y_i^2$.
\end{theorem}
\clearpage
\section{Problems}
\begin{enumerate}[label = \arabic*]
\item Let $A\in\mathbb{M}_n(\mathbb{C})$ be a complex matrix of order $n\geq 9$ and let
\[
f(x):=(x-1)^2(x-2)^3(x-3)^4
\]
Suppose that $A$ is self-adjoint and $f(A)=0$. Find all possible minimal polynomials $m_A(x)$ of $A$.
\item  Let $V$ be a finite-dimensional inner product space and $T:V\to V$ invertible. Prove that there exists a unitary operator $U$ and a positive operator $P$ on $V$ such that $T=U\circ P$.
\item AY1314Sem2 Question 6(iii)
\item AY1415Sem2 Question 8(iv) --(vi)
\item Let $V$ be a finite dimensional vector space over a field $\mathbb{F}$ and let $T$ be a linear operator on $V$. Suppose there exists $v\in V$ such that $\{\mathbf{v}, T(\mathbf{v}),\ldots, T^{n-1}(\mathbf{v})\}$ is a basis for $V$ where $n=\dim(V)$.
\begin{enumerate}
\item Prove that the linear operators $I_V, T,\ldots, T^{n-1}$ are linearly independent. (Done)
\item Let $S$ be a linear operator on $V$ such that $S\circ T = T\circ S$. Write
\[
S(\mathbf{v})=a_0\mathbf{v}+a_1T(\mathbf{v})+\cdots+a_{n-1}T^{n-1}(\mathbf{v})
\]
where $a_0,a_1,\cdots, a_{n-1}\in\mathbb{F}$.\\
Prove that $S=p(T)$ where $p(x) = a_0+a_1x+\cdots+a_{n-1}x^{n-1}$.
\item Suppose $p_T(x)=(x-\lambda_1)^{r_1}(x-\lambda_2)^{r_2}\cdots (x-\lambda_k)^{r_k}$ where $\lambda_1,\lambda_2,\ldots, \lambda_k$ are distinct eigenvalues of $T$. Find $m_T(x)$.
\end{enumerate}
\item Let $A$ be an invertible $n\times n$ matrix over a field $\mathbb{F}$.
\begin{enumerate}
\item Show that $c_{A^{-1}}(x)=x^n[c_A(0)]^{-1}c_A(\frac{1}{x})$ (Done)
\item Show that $m_{A^{-1}}(x)=x^k[m_A(0)]^{-1}m_A(\frac{1}{x})$.
\end{enumerate}
\end{enumerate}

\end{document}